#!/usr/bin/env python3
"""
`pyresid`

Python tools for mining Protein Residues from Fulltext articles using PMC number, ePMC and PDB.

author: robert.firth@stfc.ac.uk
"""

import re
import os
import requests
import json
import warnings
import gzip
import shutil
#
# import spacy as spacy
# import numpy as np
# import CifFile as CifFile
# from fuzzywuzzy import process as fwprocess
# from collections import OrderedDict
# from matplotlib import pyplot as plt
# from bs4 import BeautifulSoup
# from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
# from Bio.PDB import MMCIFParser
# from pandas import DataFrame, read_table
# from ftplib import FTP

__author__ = "Rob Firth"
__version__ = "0.5"
__all__ = ["process",
           "MyEncoder",
           "get_sections_text",
           "reconstruct_fulltext",
           "get_text",
           "aa_dict",
           "short_aa_dict",
           "setup_plot_defaults",
           "check_residue_candidate_validity",
           "identify_protein_ID",
           "locate_proteins",
           "request_fulltextXML",
           "parse_request",
           "load_protein_IDs"
           ]

mmCIF_dir = os.path.abspath(os.path.join(__file__, os.pardir, "mmCIF/"))
PDB_dir = os.path.abspath(os.path.join(__file__, os.pardir, "PDB/"))

EBI_whitelist = ["Title",
                "Abstract",
                "Introduction",
                "Methods",
                "Results",
                "Discussion",
                "Acknowledgments",
                "References",
                "Article",
                "Table",
                "Figure",
                "Case study",
                "Supplementary material",
                "Conclusion",
                "Abbreviations",
                "Competing Interests",
                "Author Contributions"
                ]

aa_dict = {"Ala": {"short_id": "A", "full_id": "Alanine", },
           "Arg": {"short_id": "R", "full_id": "Arginine", },
           "Asn": {"short_id": "N", "full_id": "Asparagine", },
           "Asp": {"short_id": "D", "full_id": "Aspartic acid (Aspartate)", },
           "Cys": {"short_id": "C", "full_id": "Cysteine", },
           "Gln": {"short_id": "Q", "full_id": "Glutamine", },
           "Glu": {"short_id": "E", "full_id": "Glutamic acid (Glutamate)", },
           "Gly": {"short_id": "G", "full_id": "Glycine", },
           "His": {"short_id": "H", "full_id": "Histidine", },
           "Ile": {"short_id": "I", "full_id": "Isoleucine", },
           "Leu": {"short_id": "L", "full_id": "Leucine", },
           "Lys": {"short_id": "K", "full_id": "Lysine", },
           "Met": {"short_id": "M", "full_id": "Methionine", },
           "Phe": {"short_id": "F", "full_id": "Phenylalanine", },
           "Pro": {"short_id": "P", "full_id": "Proline", },
           "Sec": {"short_id": "U", "full_id": "Selenocysteine"},
           "Ser": {"short_id": "S", "full_id": "Serine", },
           "Thr": {"short_id": "T", "full_id": "Threonine", },
           "Trp": {"short_id": "W", "full_id": "Tryptophan", },
           "Tyr": {"short_id": "Y", "full_id": "Tyrosine", },
           "Val": {"short_id": "V", "full_id": "Valine", },
           "Asx": {"short_id": "B", "full_id": "Aspartic acid or Asparagine", },
           "Glx": {"short_id": "Z", "full_id": "Glutamine or Glutamic acid", },
           "Xaa": {"short_id": "X", "full_id": "Any amino acid", },
           "TERM": {"short_id": None, "full_id": "termination codon", }
           }

short_aa_dict = {"A": {"id": "Ala", "full_id": "Alanine", },
                 "R": {"id": "Arg", "full_id": "Arginine", },
                 "N": {"id": "Asn", "full_id": "Asparagine", },
                 "D": {"id": "Asp", "full_id": "Aspartic acid (Aspartate)", },
                 "C": {"id": "Cys", "full_id": "Cysteine", },
                 "Q": {"id": "Gln", "full_id": "Glutamine", },
                 "E": {"id": "Glu", "full_id": "Glutamic acid (Glutamate)", },
                 "G": {"id": "Gly", "full_id": "Glycine", },
                 "H": {"id": "His", "full_id": "Histidine", },
                 "I": {"id": "Ile", "full_id": "Isoleucine", },
                 "L": {"id": "Leu", "full_id": "Leucine", },
                 "K": {"id": "Lys", "full_id": "Lysine", },
                 "M": {"id": "Met", "full_id": "Methionine", },
                 "F": {"id": "Phe", "full_id": "Phenylalanine", },
                 "P": {"id": "Pro", "full_id": "Proline", },
                 "U": {"id": "Sec", "full_id": "Selenocysteine"},
                 "S": {"id": "Ser", "full_id": "Serine", },
                 "T": {"id": "Thr", "full_id": "Threonine", },
                 "W": {"id": "Trp", "full_id": "Tryptophan", },
                 "Y": {"id": "Tyr", "full_id": "Tyrosine", },
                 "V": {"id": "Val", "full_id": "Valine", },
                 "B": {"id": "Asx", "full_id": "Aspartic acid or Asparagine", },
                 "Z": {"id": "Glx", "full_id": "Glutamine or Glutamic acid", },
                 "X": {"id": "Xaa", "full_id": "Any amino acid", },
                 "None": {"id": "TERM", "full_id": "termination codon", }
                 }

full_aa_dict = {"Alanine":{"id": "Ala", "short_id": "A"},
                "Arginine":{"id": "Arg", "short_id": "R"},
                "Asparagine":{"id": "Asn", "short_id": "N"},
                "Aspartic acid (Aspartate)":{"id": "Asp", "short_id": "D"},
                "Aspartic acid": {"id": "Asp", "short_id": "D"},
                "Aspartate": {"id": "Asp", "short_id": "D"},
                "Cysteine":{"id": "Cys", "short_id": "C"},
                "Glutamine":{"id": "Gln", "short_id": "Q"},
                "Glutamic acid (Glutamate)":{"id": "Glu", "short_id": "E"},
                "Glycine":{"id": "Gly", "short_id": "G"},
                "Histidine":{"id": "His", "short_id": "H"},
                "Isoleucine":{"id": "Ile", "short_id": "I"},
                "Leucine":{"id": "Leu", "short_id": "L"},
                "Lysine":{"id": "Lys", "short_id": "K"},
                "Methionine":{"id": "Met", "short_id": "M"},
                "Phenylalanine":{"id": "Phe", "short_id": "F"},
                "Proline":{"id": "Pro", "short_id": "P"},
                "Selenocysteine": {"id": "Sec", "short_id": "U"},
                "Serine":{"id": "Ser", "short_id": "S"},
                "Threonine":{"id": "Thr", "short_id": "T"},
                "Tryptophan":{"id": "Trp", "short_id": "W"},
                "Tyrosine":{"id":"Tyr", "short_id": "Y"},
                "Valine":{"id": "Val", "short_id": "V"},
                "Aspartic acid or Asparagine" : {"id": "Asx", "short_id": "B"},
                "Glutamine or Glutamic acid" : {"id": "Glx", "short_id": "Z"},
                "Any amino acid": {"id": "Xaa", "short_id": "X"},
                "termination codon": {"id": "TERM", "short_id": None}
                }

extract = lambda x, y: OrderedDict(zip(x, map(y.get, x)))


def setup_plot_defaults():
    """

    Sets up default plot settings for figures.

    Parameters
    ----------


    Returns
    -------

    """

    plt.rcParams['ps.useafm'] = True
    plt.rcParams['pdf.use14corefonts'] = True
    plt.rcParams['text.usetex'] = True
    plt.rcParams['font.size'] = 14
    plt.rcParams['figure.subplot.hspace'] = 0.1
    plt.rc('font', family='sans-serif')
    plt.rc('font', serif='Helvetica')
    pass


class MyEncoder(json.JSONEncoder):
    """

    """

    def default(self, obj):
        if isinstance(obj, OrderedDict):
            return obj
        # elif isinstance(obj, MatchClass):
        #     return dict(MatchClass.__dict__)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, spacy.tokens.token.Token):
            return str(obj)
        else:
            return super(MyEncoder, self).default(obj)


class MatchClass:
    """
    Class for handling residue matches.
    """

    def __init__(self, start, end, string):
        """

        Parameters
        ----------
        start :
        end :
        string :
        """

        self.start = start
        self.end = end
        self.string = string

    def translate(self):
        pass

    def find_position(self):
        """
        Find number within string that has been matched

        Returns
        -------

        """

        # Find number within string
        number_pattern = "\d+"
        p = re.compile(number_pattern)
        # result = p.search(self.string)
        result = p.findall(self.string)

        # self.position = int(result.group())  ## Convert string to integer
        self.position = result

        return result

    def find_amino_acid(self):
        """

        Returns
        -------

        """

        # print(self.string)

        ## Residue Position
        ## Line 1: Standard Positions, Line 2: Positions in parentheses, Line 3: Positions preceded by a -
        # pattern_POS = "((((\d+[\),.\s'\"])|(\d+\Z)))" + \
        #               "|((\(\d+[\),.\s'\"])|(\d+\)\Z))" + \
        #               "|(((-\d+[\),.\s'\"])|(-\d+\Z)))" + \
        #               ")"
        pattern_POS = "((\d+)" + \
                      "|(((\d+[\),.\s'\"])|(\d+\Z)))" + \
                      "|((\(\d+[\),.\s'\"])|(\d+\)\Z))" + \
                      "|(((-\d+[\),.\s'\"])|(-\d+\Z)))" + \
                      ")"

        ## Residue Name-Full
        pattern_RESNF = r"[aA]lanine|[aA]rginine|[aA]sparagine|[aA]spartate|[aA]spartateic [aA]cid|[cC]ysteine|[gG]lutamine|[gG]lutamate|[gG]lutamic [aA]cid|[gG]lycine|[hH]istidine|[iI]soleucine|[lL]eucine|[lL]ysine|[mM]ethionine|[pP]henylalanine|[pP]roline|[sS]erine|[tT]hreonine|[tT]ryptophan|[tT]yrosine|[vV]aline|[pP]yrrolysine|[sS]elenocysteine|[aA]spartic [aA]cid|[aA]sparagine|[gG]lutamic [aA]cid|[gG]lutamine"
        ## Residue Name-1 Letter
        pattern_RESN1 = "[ARNDCQEGHILKMFPSTWYVOUBZX]"
        ## Residue Name-3 Letter
        pattern_RESN3 = "([aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa)"
        ## Combine
        pattern_WTRES = "(" + pattern_RESNF + "|" + pattern_RESN3 + ")"


        ## Best Case Scenario
        pattern_simple = "(" + pattern_WTRES + pattern_POS + ")|((\Z)" + pattern_WTRES + pattern_POS + ")"

        pattern_standard_join_slash = "((" + pattern_RESN3 + ")(\d+\/))+((" + pattern_RESN3 + ")(" + pattern_POS + "))"
        pattern_standard_join_slash_bracket = "(" + pattern_WTRES + "\(\d+\)/)+" + "(" + pattern_WTRES + "\(\d+\))"
        pattern_standard_join_slash_dash = "(" + pattern_WTRES + "-\d+/)+" + "(" + pattern_WTRES + "-\d+)"

        ## Residue Name-3 Letter Brackets
        # pattern_RESN3b = "([aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA)(\(\d+\))"
        pattern_RESN3b = "([aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa)(\(\d+\))"
        ##
        ## Residue Name-3 Letter repeating dashes
        # pattern_RESN3dr = "(([,\s]((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))|((\(((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))"
        pattern_RESN3_dash_repeat = "((([,\s\'\"\(])|(\A))((" + pattern_RESN3 + ")(-\d+|\d+)-)+)((" + pattern_RESN3 + ")(-\d+|\d+))"

        ##
        pattern_dash = pattern_RESN3 + "-" + pattern_POS
        ##
        pattern_standard = "(" + pattern_RESNF + ") " + pattern_POS + "|((" + pattern_RESN3 + ")( " + pattern_POS + \
                           "))|(" + pattern_RESN3 + ")(" + pattern_POS + ")|(,(" + pattern_RESN3 + ")" + pattern_POS + \
                           ")|(" + pattern_RESN3b + ")"
        ##
        pattern_site = "(" + pattern_standard + ") residue?" + \
                       "|(" + pattern_RESNF + "|" + pattern_RESN3 + ")" + "? at position? " + pattern_POS + \
                       "|(residue at position " + pattern_POS + ")" + \
                       "|((" + pattern_RESN3 + ")\sresidues\sat\spositions\s\d+ and \d+)" + \
                       "|((" + pattern_RESN3 + ")\sresidues\sat\spositions\s(\d+,)+\d+ and \d+)"

        ## Try fullname first
        p = re.compile(pattern_RESNF)
        result = p.search(self.string) ## TODO WILL NEED DIFFERENT TREATMENT FOR MUTANTS - TODO - USE match GROUPS to identify which capturing group is whichs?

        if result is not None:
            self.aminoacid = result.group().title()
            self.threeletter = full_aa_dict[self.aminoacid]["id"]
            return self.aminoacid

        ## Find Natural mentions
        p = re.compile(pattern_site)
        result = p.search(self.string)
        # print(result)
        if result is not None:
            # print("pattern site self")
            p = re.compile(pattern_RESNF)
            result = p.search(self.string)
            if result is not None:
                aminoacid = "".join(filter(str.isalpha, result.group())).title()
                self.aminoacid = full_aa_dict[aminoacid]["full_id"]
                self.threeletter = self.aminoacid

                return self.aminoacid

            p = re.compile(pattern_RESN3)
            result = p.search(self.string)
            if result is not None:
                aminoacid = "".join(filter(str.isalpha, result.group())).title()
                self.aminoacid = aa_dict[aminoacid]["full_id"]
                self.threeletter = aminoacid

                return self.aminoacid


        ## Try threeletters + number / threeletters + number
        p = re.compile(pattern_standard_join_slash)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            pattern = "(" + pattern_RESN3 + ")\d+"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid

        ## Try threeletters + dash + number / threeletters + dash + number
        p = re.compile(pattern_standard_join_slash_dash)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            pattern = "(" + pattern_RESN3 + ")-\d+"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid


        ## Try threeletters + bracketed number / threeletters + bracketed number
        p = re.compile(pattern_standard_join_slash_bracket)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            pattern = "(" + pattern_RESN3 + ")\d+"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string.replace("(", "").replace(")", "")):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid

        ## Try threeletters + number (only carry threeletters) by replacing "-" (threeletter + dash + number)
        # p = re.compile(pattern_RESN3dr)
        p = re.compile(pattern_RESN3_dash_repeat)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            # pattern = "(" + pattern_RESN3 + ")\d+"
            pattern = pattern_WTRES+"((-\d+)|(\d+))"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid
        ## TODO Brackets too?

        ## Try threeletters + number (only carry threeletters)
        p = re.compile(pattern_RESN3)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = aa_dict[aminoacid]["full_id"]
            self.threeletter = aminoacid

            return self.aminoacid

        ## Try oneletter + number (only carry oneletter)
        p = re.compile(pattern_RESN1)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = short_aa_dict[aminoacid]["full_id"]
            self.threeletter = short_aa_dict[aminoacid]["id"]

            return self.aminoacid

        ## Try threeletter + bracketed number
        p = re.compile(pattern_RESN3b)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = aa_dict[aminoacid]["full_id"]
            self.threeletter = aminoacid

            return self.aminoacid

        pass

    def _find_amino_acid(self):
        """

        Returns
        -------

        """

        # print(self.string)

        ## Residue Position
        pattern_POS = "\d+"
        ## Residue Name-Full
        pattern_RESNF = r"[aA]lanine|[aA]rginine|[aA]sparagine|[aA]spartate|[aA]spartateic [aA]cid|[cC]ysteine|[gG]lutamine|[gG]lutamate|[gG]lutamic [aA]cid|[gG]lycine|[hH]istidine|[iI]soleucine|[lL]eucine|[lL]ysine|[mM]ethionine|[pP]henylalanine|[pP]roline|[sS]erine|[tT]hreonine|[tT]ryptophan|[tT]yrosine|[vV]aline|[pP]yrrolysine|[sS]elenocysteine|[aA]spartic [aA]cid|[aA]sparagine|[gG]lutamic [aA]cid|[gG]lutamine"
        ## Residue Name-1 Letter
        pattern_RESN1 = "([ARNDCQEGHILKMFPSTWYVOUBZX])\d+"
        ## Residue Name-3 Letter
        # pattern_RESN3 = "[aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA"
        pattern_RESN3 = "[aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa"
        ##
        pattern_standard_join_slash = "((" + pattern_RESN3 + ")(\d+\/))+((" + pattern_RESN3 + ")(" + pattern_POS + "))"
        ## Residue Name-3 Letter Brackets
        # pattern_RESN3b = "([aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA)(\(\d+\))"
        pattern_RESN3b = "([aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa)(\(\d+\))"
        ##
        ## Residue Name-3 Letter repeating dashes
        pattern_RESN3dr = "(([,\s]((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))|((\(((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))"
        ##
        pattern_dash = pattern_RESN3 + "-" + pattern_POS
        ##
        pattern_standard = "(" + pattern_RESNF + ") " + pattern_POS + "|((" + pattern_RESN3 + ")( " + pattern_POS + \
                           "))|(" + pattern_RESN3 + ")(" + pattern_POS + ")|(,(" + pattern_RESN3 + ")" + pattern_POS + \
                           ")|(" + pattern_RESN3b + ")"
        ##
        pattern_site = "(" + pattern_standard + ") residue?" + \
                       "|(" + pattern_RESNF + "|" + pattern_RESN3 + ")" + "? at position? " + pattern_POS + \
                       "|(residue at position " + pattern_POS + ")" + \
                       "|((" + pattern_RESN3 + ")\sresidues\sat\spositions\s\d+ and \d+)" + \
                       "|((" + pattern_RESN3 + ")\sresidues\sat\spositions\s(\d+,)+\d+ and \d+)"

        ## Try fullname first
        p = re.compile(pattern_RESNF)
        result = p.search(self.string)

        if result is not None:
            self.aminoacid = result.group().title()
            self.threeletter = full_aa_dict[self.aminoacid]["id"]
            return self.aminoacid

        ## Find Natural mentions
        p = re.compile(pattern_site)
        result = p.search(self.string)
        # print(result)
        if result is not None:
            # print("pattern site self")
            p = re.compile(pattern_RESNF)
            result = p.search(self.string)
            if result is not None:
                aminoacid = "".join(filter(str.isalpha, result.group())).title()
                self.aminoacid = full_aa_dict[aminoacid]["full_id"]
                self.threeletter = self.aminoacid

                return self.aminoacid

            p = re.compile(pattern_RESN3)
            result = p.search(self.string)
            if result is not None:
                aminoacid = "".join(filter(str.isalpha, result.group())).title()
                self.aminoacid = aa_dict[aminoacid]["full_id"]
                self.threeletter = aminoacid

                return self.aminoacid


        ## Try threeletters + number / threeletters + number
        p = re.compile(pattern_standard_join_slash)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            pattern = "(" + pattern_RESN3 + ")\d+"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid

        ## Try threeletters + number (only carry threeletters) by replacing "-" (threeletter + dash + number)
        p = re.compile(pattern_RESN3dr)
        result = p.search(self.string)
        if result is not None:
            self.aminoacid = []
            self.threeletter = []
            pattern = "(" + pattern_RESN3 + ")\d+"
            p = re.compile(pattern)

            for submatch in p.finditer(self.string):
                aminoacid = "".join(filter(str.isalpha, submatch.group())).title()
                self.aminoacid.append(aminoacid)
                self.threeletter.append(aminoacid)

            return self.aminoacid


        ## Try threeletters + number (only carry threeletters)
        p = re.compile(pattern_RESN3)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = aa_dict[aminoacid]["full_id"]
            self.threeletter = aminoacid

            return self.aminoacid

        ## Try oneletter + number (only carry oneletter)
        p = re.compile(pattern_RESN1)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = short_aa_dict[aminoacid]["full_id"]
            self.threeletter = short_aa_dict[aminoacid]["id"]

            return self.aminoacid

        ## Try threeletter + bracketed number
        p = re.compile(pattern_RESN3b)
        result = p.search(self.string)
        if result is not None:
            aminoacid = "".join(filter(str.isalpha, result.group())).title()
            self.aminoacid = aa_dict[aminoacid]["full_id"]
            self.threeletter = aminoacid

            return self.aminoacid

        pass

    def encode(self):
        if isinstance(self.position, list):

            warnings.warn("\n self.position is a list, not a string! \n If the list is 1-element long, I'll try and work it out")

            if len(self.position) == 1:
                self.position = self.position[0]

        self.residue = self.threeletter + str(self.position)


class ProteinMatchClass:
    """
    Class for handling protein structure matches
    """
    def __init__(self, start, end, string):
        """

        Parameters
        ----------
        start
        end
        string
        """

        self.start = start
        self.end = end
        self.string = string


class SectionMatchClass:
    """
    Class for handling source section matches
    """

    def __init__(self, start, end, start_token_number, start_token, end_token_number, end_token, title, EBI_title):
        """

        Parameters
        ----------
        start :

        end :

        start_token_number :

        start_token :

        title :

        """

        self.start = start # char
        self.end = end # char
        self.start_token_number = start_token_number
        self.start_token = start_token
        self.end_token_number = end_token_number
        self.end_token = end_token
        self.title = title
        self.EBI_title = EBI_title


class SourceClass:
    """
    Class for handling sources


    """
    def __init__(self):
        """

        """
        self.ext_id = None
        self.text_dict = None
        self.sections = None
        self.fulltext = None
        self.doc = None
        pass


    def get_section_matches(self, verbose=False):
        """

        Parameters
        ----------

        verbose :


        Returns
        -------

        """
        sections = []

        indices = [token.idx for token in self.doc]

        for i, key in enumerate(self.text_dict):

            start_char = self.text_dict[key]["offset"]
            start_token_number = np.digitize(start_char, indices) - 1
            start_token = self.doc[start_token_number]

            if i < len(self.text_dict) - 1:
                end_char = self.text_dict[list(self.text_dict.keys())[i + 1]]["offset"] - 1
            else:
                end_char = len(self.fulltext)

            end_token_number = np.digitize(end_char, indices) - 1
            end_token = self.doc[end_token_number]


            if verbose:
                print(i, self.text_dict[key]["title"], self.text_dict[key]["offset"], end_char,
                      np.digitize(start_char, indices) - 1)

            title = self.text_dict[key]["title"]
            fuzzymatch = fwprocess.extract(title, EBI_whitelist, limit=1)[0]
            EBI_title = fuzzymatch[0]

            section = SectionMatchClass(start_char, end_char, start_token_number, start_token, end_token_number, end_token, title, EBI_title)

            sections.append(section)

        self.section_matches = sections

        pass


def request_fulltextXML(ext_id):
    """
    Requests a fulltext XML document from the ePMC REST API. Raises a warning if this is not
    possible

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------
    r : `Requests.Response <http://docs.python-requests.org/en/master/api/#requests.Response>`_
        The response to the query served up by the requests package.
    """
    request_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/" + ext_id + "/fullTextXML"

    r = requests.get(request_url)

    if r.status_code == 200:
        return r
    else:
        warnings.warn("request to " + str(ext_id) + " has failed to return 200, and has returned " + str(r.status_code))
        return r
    pass


def parse_request(ext_id):
    """
    Wrapper for :func:`~pyresid.request_fulltextXML` that returns a `BeautifulSoup` XML object

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------

    soup : `BeautifulSoup <https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html#beautifulsoup>`_
        BeautifulSoup XML object created from the text response from  :func:`pyresid.request_fulltextXML`


    See Also
    --------
    :func:`~pyresid.request_fulltextXML`

    """
    r = request_fulltextXML(ext_id=ext_id)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, "lxml-xml")
    else:
        warnings.warn("request to " + str(ext_id) + " has failed to return 200, and has returned " + str(r.status_code))
        return r

    return soup


def get_metadata(ext_id):
    """
    Query EBI ePMC API, extract "article-meta"

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------
    meta_dict : Dict
            Dictionary containing metadata, including the title and authors
    """
    soup = parse_request(ext_id=ext_id)

    meta = soup.findAll("article-meta")

    meta_dict = {}
    meta_dict["title"] = meta[0].find(re.compile("^title")).text
    meta_dict["authors"] = []

    for author in meta[0].findAll("contrib"):
        auth_dict = {}

        auth_dict["given_name"] = author.find("given-names").text
        auth_dict["surname"] = author.find("surname").text
        meta_dict["authors"].append(auth_dict)

    return (meta_dict)


def get_all_text(ext_id):
    """
    Wrapper for `~pyresid.parse_request`

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------
    soup : `BeautifulSoup <https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html#beautifulsoup>`_
        BeautifulSoup XML object created from the text response from  :func:`pyresid.request_fulltextXML`

    """

    return parse_request(ext_id=ext_id).get_text(" ")


def get_sections_text(ext_id, remove_tables=True, fulltext=False, verbose=False):
    """
    Requests fulltext XML from the EBI ePMC web REST API, parses the response into a dict.

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    remove_tables : Bool, optional, default: True
                Flag to ignore the text found within tables.

    fulltext : Bool, optional, default: False
           Flag used to return a 'dumb' fulltext (rather than that
           from :func:`~pyresid.reconstruct_fulltext`)

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    text_dict : OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML (technically a child of the `<body>`).

    See Also
    --------
    * :func:`~pyresid.request_fulltextXML`
    * :func:`~pyresid.parse_request`
    * :func:`~pyresid.reconstruct_fulltext`

    """

    soup = parse_request(ext_id=ext_id)

    if remove_tables:
        for i in range(0, len(soup('table'))):  # https://stackoverflow.com/q/18934136
            soup.table.decompose()

    if fulltext:
        return " ".join([sec.get_text(" ") for sec in soup.findAll("sec")])
    else:
        text_dict = OrderedDict()

    for sec_number, sec in enumerate(soup.find("body").children):

        title = sec.find("title")

        if "id" in sec.attrs:
            if verbose: print("hasid")
            sec_id = sec["id"]
        if title:
            if verbose: print("hastitle")
            sec_id = title.text.strip()

        if "id" not in sec.attrs and not title:
            warnings.warn("cannot id the section")
            sec_id = str(sec_number)

        if verbose: print(sec_id, end="")

        if title:
            title = title.text.strip()
        else:
            title = sec_id
        if sec_id not in text_dict:
            text_dict[sec_id] = {"title": title, "text": sec.get_text(" ")}
        else:
            i = 1
            while sec_id not in text_dict:
                sec_id = sec_id + str(i)
                text_dict[sec_id] = {"title": title, "text": sec.get_text(" ")}
                i += 1
        if verbose: print("")

    return text_dict


def _deprecated_get_sections_text(ext_id, remove_tables=True, fulltext=False):
    """
    DEPRECATED

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    remove_tables : Bool, optional, default: True
                Flag to ignore the text found within tables.

    fulltext : Bool, optional, default: False
           Flag used to return a 'dumb' fulltext (rather than that
           from :func:`~pyresid.reconstruct_fulltext`)

    Returns
    -------
    text_dict : OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML (technically a child of the `<body>`).
    """
    soup = parse_request(ext_id=ext_id)

    if remove_tables:
        for i in range(0, len(soup('table'))):  # https://stackoverflow.com/q/18934136
            soup.table.decompose()

    if fulltext:
        return " ".join([sec.get_text(" ") for sec in soup.findAll("sec")])
    else:
        text_dict = OrderedDict()
        for sec in soup.findAll("sec"):
            if not sec.find_parent("sec"):  # https://stackoverflow.com/a/31208775
                try:
                    sec_id = sec["id"]
                    title = sec.find("title")

                    if title:
                        title = title.text

                    text_dict[sec_id] = {"title": title, "text": sec.get_text(" ")}

                except:
                    pass

        return text_dict


def get_text(ext_id, verbose=False):
    """
    A wrapper for :func:`~pyresid.get_sections_text` that adds additional information
    to the *text_dict*.

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    text_dict : OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML (technically a child of the `<body>`). An augmented version of
            the *text_dict* returned by :func:`~pyresid.get_sections_text` - containing
            additional information including spaCy tokens, length in characters and
            starting offset.

    See Also
    --------
    * :func:`~pyresid.get_sections_text`
    """

    text_dict = get_sections_text(ext_id=ext_id, remove_tables=True, fulltext=False)

    nlp = spacy.load('en')

    for i, sec in enumerate(text_dict):
        if verbose:
            print("This sec=", sec)

        doc = nlp(text_dict[sec]["text"])
        text_dict[sec]["tokens"] = [t for t in doc]
        text_dict[sec]["len"] = len(
            text_dict[sec]["tokens"])  ## this len is number of tokens. Also want number of chars
        text_dict[sec]["char_len"] = len(text_dict[sec]["text"])
        if verbose:
            print("char_len is ", text_dict[sec]["char_len"])
        if i == 0:
            text_dict[sec]["start"] = 0
            text_dict[sec]["offset"] = 0
        else:
            text_dict[sec]["start"] = text_dict[list(text_dict.keys())[i - 1]]["start"] + \
                                      text_dict[list(text_dict.keys())[i - 1]]["len"]
            text_dict[sec]["offset"] = text_dict[list(text_dict.keys())[i - 1]]["offset"] + \
                                       text_dict[list(text_dict.keys())[i - 1]]["char_len"]

            if verbose:
                print("Previous sec = ", list(text_dict.keys())[i - 1])
                print("Previous sec char_len = ", text_dict[list(text_dict.keys())[i - 1]]["char_len"])

    return text_dict


def query_ID_converter(ext_id):
    """
    Converts ePMC ext_id into PMID , API description here - https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------
    response_json : dict
                json returned by the API containing the relevant information. Can be passed to
                :func:`~pyre.convert_PMCID_to_PMID`

    See Also
    --------
    * :func:`~pyre.convert_PMCID_to_PMID`
    """

    service_root_url = "https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids="

    request_url = service_root_url + ext_id

    fmt = "json"
    request_url = request_url + r"&format=" + fmt

    tool = "pyresid"
    request_url = request_url + r"&tool=" + tool

    email = "robert.firth@stfc.ac.uk"
    request_url = request_url + r"&email=" + email

    r = requests.get(request_url)
    response_json = json.loads(r.text)

    return response_json


def convert_PMCID_to_PMID(ext_id):
    """
    Converts ePMC ID into PubMed ID

    Parameters
    ----------
    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    Returns
    -------
    pmid : string
       PubMed URI corresponding to the PMC ID URI supplied

    """
    data_obj = query_ID_converter(ext_id=ext_id)
    pmid = data_obj["records"][0]["pmid"]
    return pmid


def convert_PMID_to_PMCID(pmid):
    """
    Converts ePMC ID into PubMed ID

    Parameters
    ----------
    ext_id : String
         PubMed UI used to retrieve the relevant entry. Format is a string representation of an integer.

    Returns
    -------
    pmid : string
       PMC URI corresponding to the PMC ID URI supplied

    """
    data_obj = query_ID_converter(ext_id=pmid)
    pmid = data_obj["records"][0]["pmcid"]
    return pmid


def reconstruct_fulltext(text_dict, tokenise=True, verbose=False):
    """
    Converts a *text_dict* into a single string or series of tokens in a list of strings.

    Parameters
    ----------
    text_dict : Dict or OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML. Usually an output from :func:`~pyresid.get_sections_text`

    tokenise : Bool, optional, default: False
           Flag to enable or disable the reconstructed text being returned as spaCy tokens
           rather than a string

    verbose : Bool, optional, default: False
          Flag to turn on verbose output


    Returns
    -------

    fulltext : string
           text snippet to be searched for residues.

    OR

    fulltext_tokens : List of strings
                  Array of tokens that can be used, for example, to search for
                  *residue_mentions*.

    See Also
    --------
    :func:`~pyresid.get_sections_text`

    """

    fulltext = " ".join([text_dict[sec]["text"] for sec in text_dict])

    if tokenise:
        if verbose:
            print("Tokenising...")
        nlp = spacy.load('en')
        fulldoc = nlp(fulltext)
        fulltext_tokens = [t.text for t in fulldoc]

        return fulltext_tokens
    else:
        return fulltext


def _identify_residues(fulltext, return_mentions=False, short=False, verbose=False):
    """
    Identify the residues that are mentioned within a string of text.

    Parameters
    ----------
    fulltext :  string
                text snippet to be searched for residues.

    return_mentions : Bool, optional, default: False
                      Flag passed to determine whether to return the residue that matches, or the specific
                      mention that triggers the identification. e.g. for the string "Arg123/125", with
                      *return_mentions* =False would return {"Arg123", "Arg125"}, while *return_mentions* =True
                      would return {"Arg123/125"}.

    short : Bool, optional, default: False
            Enable the matching of "short" Amino-Acid codes, such as "A123", as well as the three-letter
            default nomenclature.

    verbose : Bool, optional, default: False
              Flag to turn on verbose output

    Returns
    -------

    unique_mentions : Set
                      Matches found within *fulltext*

    See Also
    --------
    * `pyresid.aa_dict` : Dictionary with three letter Amino-Acid identifiers as keys, and items that allow translation to short identifiers or full names.
    * `pyresid.short_aa_dict` : Dictionary with short Amino-Acid identifiers as keys, and items that allow translation to three letter codes or full names.

    """

    prefixes = list(aa_dict.keys())

    pattern = "[a-zA-Z]{3}\d+/\d+|[a-zA-Z]{3}\d+"

    p = re.compile(pattern)

    unique_mentions = set([i.strip() for i in p.findall(fulltext) if "".join(filter(str.isalpha, i)) in prefixes])

    if short:
        short_prefixes = list(short_aa_dict.keys())
        pattern = r"\b[a-zA-Z]\d+\b"
        p = re.compile(pattern)
        unique_short_mentions = set(
            [i.strip() for i in p.findall(fulltext) if "".join(filter(str.isalpha, i)) in short_prefixes])

        unique_mentions.update(unique_short_mentions)

    if return_mentions:
        return unique_mentions

    unique_residues = []
    for i in unique_mentions:
        if verbose:
            print(i)
        if "/" in i:
            residue_position = ["".join(filter(str.isdigit, j)) for j in i.split("/")]
            aa_residue_id = [j for j in ["".join(filter(str.isalpha, i)) for i in i.split("/")] if j]
            decomposed = [x + y for x in aa_residue_id for y in
                          residue_position]  ## TODO - https://stackoverflow.com/a/19560204
            unique_residues += decomposed
        else:
            unique_residues.append(i)

    return set(unique_residues)


def check_residue_candidate_validity(cand, pattern="[a-zA-Z]{3}\d+/\d+|[a-zA-Z]{3}\d+", verbose=False):
    """

    Parameters
    ----------
    cand : String
       Candidate residue to check for validity

    pattern : String, optional, default : "[a-zA-Z]{3}\d+/\d+|[a-zA-Z]{3}\d+".
          Regular Expression with which to match the candidate.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    match_bool_list : List of Bool.
                  List of True/False booleans reflecting the validity of the input candidates.

    """
    if type(cand) in {np.str, str} or not hasattr(cand, "__iter__"):
        if verbose:
            print(cand)
        match = re.match(pattern, cand.strip())

        if match:
            if verbose:
                print("Format Match")

            isolated_aa = "".join(filter(str.isalpha, cand)).strip()

            if verbose:
                print("isolatedAA = ", isolated_aa)

            if isolated_aa.lower() in [i.lower() for i in aa_dict.keys()]:
                if verbose:
                    print("OK")

                return True

            else:
                if verbose:
                    print("Not OK")
                return False

        else:
            if verbose:
                print("Not OK, format mismatch")

            return False
    else:
        match_bool_list = []
        for candidate in cand:
            if verbose:
                print(candidate)
            match_bool_list.append(check_residue_candidate_validity(candidate))
        return match_bool_list


def decompose_slashed_residue(i):
    """

    Parameters
    ----------
    i :

    Returns
    -------

    """
    # if "/" in i:
    #     residue_position = ["".join(filter(str.isdigit, j)) for j in i.split("/")]
    #     aa_residue_id = [j for j in ["".join(filter(str.isalpha, i)) for i in i.split("/")] if j]
    #     decomposed = [x + y for x in aa_residue_id for y in residue_position] ## TODO doesn't do what I think
    #     return decomposed
    # else:
    #     return [i,]
    return list(_identify_residues(i))


def subdivide_compound_residues(instring, verbose=False):
    """

    Parameters
    ----------
    instring : String
           Input compound candidate to decompose into a number of actual residue ids.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    """

    if verbose: print(instring)
    cands = instring.split("/")
    validity = check_residue_candidate_validity(cands)
    sublists = []
    x = []  ## Need this in case element 1 is not valid?

    for i, boolean_value in enumerate(validity):
        if verbose: print(i, boolean_value)
        if boolean_value:

            if i > 0:
                sublists.extend(decompose_slashed_residue("/".join(x)))

            x = [cands[i], ]

        else:

            x.append(cands[i])

    sublists.extend(decompose_slashed_residue("/".join(x)))
    return (sublists)


def identify_protein_ID(fulltext, simple=False, infile=None, locdir=None, verbose=False):
    """
    Uses Regular Expressions to find Protein IDs in the, and then cross-checks against the PDB list
    of entities.

    Parameters
    ----------
    fulltext :  string
            text snippet to be searched for residues.

    simple : Bool, optional, default: False
         Flag that, if set, will skip the check against the PDB. Generally a bad idea.

    infile : String or Path, optional, None
         File to load in order to check candidate PDB entries against. contains the PDB entries -
         if `None` defaults to "PDBID.list"

    locdir : String or Path, optional, default: None
         Directory that contains the infile. If None, default is assumed to be `PDB_dir`, if this
         is not found then will be the user home.

    verbose : Bool, optional, default: False
              Flag to turn on verbose output

    Returns
    -------
    unique_protiens : Set
                  Matches found within `fulltext`
    See Also
    --------
    :func:`~pyresid._identify_residues`
    :func:`~pyresid.load_protein_IDs`
    """

    ## TODO - Case insensitive - missed all pdb entries in PMC5297931

    # pattern = r"\b[0-9][A-Z0-9]{3}\b"
    pattern = r"\b[0-9][a-zA-Z0-9]{3}\b"

    p = re.compile(pattern)

    pdb_id_candidates = p.findall(fulltext)

    if verbose:
        print(pdb_id_candidates)

    unique_mentions = set(pdb_id_candidates)

    if simple:
        return list(unique_mentions)

    unique_protiens = []
    pdb_ids = load_protein_IDs(infile=infile, locdir=locdir)

    if verbose:
        print("Checking against PDB.")

    for candidate in unique_mentions:
        if candidate.upper() in pdb_ids:
            if verbose:
                print(candidate)

            unique_protiens.append(candidate)

    return unique_protiens


def locate_proteins(fulltext, simple=False, infile=None, locdir=None, verbose=False):
    """
    Uses Regular Expressions to find Protein IDs in the, and then cross-checks against the PDB list
    of entities. Returns a list of MatchClass objects, rather than a list of strings, as in

    Parameters
    ----------
    fulltext :  string
            text snippet to be searched for residues.

    simple : Bool, optional, default: False
         Flag that, if set, will skip the check against the PDB. Generally a bad idea.

    infile : String or Path, optional, None
         File to load in order to check candidate PDB entries against. contains the PDB entries -
         if `None` defaults to "PDBID.list"

    locdir : String or Path, optional, default: None
         Directory that contains the infile. If None, default is assumed to be `PDB_dir`, if this
         is not found then will be the user home.

    verbose : Bool, optional, default: False
              Flag to turn on verbose output

    Returns
    -------
    matches : List of :class:`~pyresid.MatchClass` objects
        The matches found within `fulltext`.
    See Also
    --------
    * :func:`~pyresid.identify_residues`
    * :func:`~pyresid.load_protein_IDs`
    """

    ## TODO - Case insensitive - missed all pdb entries in PMC5297931
    # pattern = r"\b[0-9][A-Z0-9]{3}\b"
    pattern = r"\b[0-9][a-zA-Z0-9]{3}\b"

    p = re.compile(pattern)

    pdb_id_candidates = p.findall(fulltext)

    if verbose:
        print(pdb_id_candidates)

    unique_mentions = set(pdb_id_candidates)

    unique_protiens = []
    pdb_ids = load_protein_IDs(infile=infile, locdir=locdir)

    if verbose:
        print("Checking against PDB.")

    for candidate in unique_mentions:
        if candidate.upper() in pdb_ids:
            if verbose:
                print(candidate)

            unique_protiens.append(candidate)

    ##Tests
    matches = []

    for match in p.finditer(fulltext):

        if verbose:
            print(match.start(), match.end(), match.group())
        if match.group() in unique_protiens:
            if verbose:
                print("valid")
            m = MatchClass(match.start(), match.end(), match.group())
            matches.append(m)
        else:
            if verbose:
                print("not valid")

    return matches


def _locate_proteins(fulltext_tokens, text_dict, unique_proteins, verbose=False):
    """

    Parameters
    ----------
    fulltext_tokens : List of strings
                  Array of tokens to search for `residue_mentions`, typically
                  the output from :func:`~pyresid.reconstruct_fulltext`.

    text_dict : Dict or OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML. Usually an output from :func:`~pyresid.get_sections_text`

    unique_proteins : Set, or other iterable.
                   Typically output from :func:`~pyresid.identify_protein_ID`,
                   the protein structures which to locate within `fulltext_tokens`.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    location_dict : OrderedDict
                Ordered dictionary containing the locations, matched string
                and frequency of each of the protein structure mentions passed, which
                are themselves used as the dict keys.
    See Also
    --------
    * :func:`~pyresid._locate_residues`
    * :func:`~pyresid.identify_protein_ID`
    """

    sec_names = [text_dict[i]["title"] for i in text_dict]
    sec_offset = [text_dict[i]["offset"] for i in text_dict]

    location_dict = OrderedDict()

    for tkn in unique_proteins:

        if tkn not in location_dict:
            location_dict[tkn] = OrderedDict()

            location_dict[tkn]["offset"] = []
            location_dict[tkn]["locations"] = []
            location_dict[tkn]["string"] = []
            location_dict[tkn]["freq"] = 0

        if verbose:
            print(tkn)

        locations = [word for word in enumerate(fulltext_tokens) if
                     tkn in str(word[1])]
        offsets = [word[1].idx for word in locations]
        if verbose:
            print(locations)

        location_dict[tkn]["offset"] += [word[1].idx for word in locations]
        location_dict[tkn]["locations"] += list(np.array(locations).T[0].astype(int))
        location_dict[tkn]["string"] += list(np.array(locations).T[1])

        location_dict[tkn]["freq"] += len(locations)

    return location_dict


def load_protein_IDs(infile=None, locdir=None):
    """
    Reads in list of valid (approved and pending) PDB entries.

    Parameters
    ----------
    infile : String or Path, optional, default: None
         File that contains the PDB entries - defaults to "PDBID.list"

    locdir : String or Path, optional, default: None
         Directory that contains the infile. If None, default is assumed to be `PDB_dir`, if this
         is not found then will be the user home.

    Returns
    -------
    pdb_arr : List of strings
        List of valid (approved and pending) PDB entries

    See Also
    --------
    * :func:`pyresid.combine_compound_IDs`
    * :func:`pyresid.get_compound_IDfiles`

    """
    if not locdir:
        if os.path.exists(PDB_dir):
            locdir = PDB_dir
        if "HOME" in os.environ:
            locdir = os.environ["HOME"]
        elif "HOMEPATH" in os.environ:
            locdir = os.environ["HOMEPATH"]
    if not infile:
        infile = os.path.join(locdir, "PDBID.list")

    pdb_arr = list(np.loadtxt(infile, dtype=str))

    return pdb_arr


def combine_compound_IDs(source_infile=None, pending_infile=None, outfile=None, locdir=None, outdir=None,
                         save=True, return_df=False):
    """

    Parameters
    ----------
    source_infile :

    pending_infile :

    outfile :

    locdir :

    outdir :

    save :

    return_df :


    Returns
    -------

    """

    if not locdir:
        if "HOME" in os.environ:
            locdir = os.environ["HOME"]
        elif "HOMEPATH" in os.environ:
            locdir = os.environ["HOMEPATH"]

    if not source_infile:
        source_infile = os.path.join(locdir, "FTP_source.idx")
    if not pending_infile:
        pending_infile = os.path.join(locdir, "FTP_pending.list")

    current_prot_df = read_table(source_infile, skiprows=4, header=None, names=["PDBID", "name"])

    idcodes = []
    description = []
    next_line_is_description_better_set_a_flag = False

    with open(pending_infile, "r") as infile:
        for line in infile:
            try:
                if next_line_is_description_better_set_a_flag:
                    description.append(line)
                    next_line_is_description_better_set_a_flag = False

                if line.split()[0] == "Idcode:":
                    #                 print(line)
                    idcodes.append(line.split()[1])
                if line.split()[0] == "Entry":
                    next_line_is_description_better_set_a_flag = True

            except:
                pass

    pending_prot_df = DataFrame(list(zip(idcodes, description)), columns=["PDBID", "name"])

    prot_df = current_prot_df.append(pending_prot_df)

    if save:
        if not outdir:
            if "HOME" in os.environ:
                outdir = os.environ["HOME"]
            elif "HOMEPATH" in os.environ:
                outdir = os.environ["HOMEPATH"]
        if not outfile:
            outfile = os.path.join(outdir, "PDBID.list")

        prot_df["PDBID"].to_csv(outfile, index=False)

    if return_df:
        return prot_df
    else:
        pass


def get_compound_IDfiles(ftp_url="ftp.wwpdb.org", outdir=None, pending=True):
    """

    Parameters
    ----------
    ftp_url :

    outdir :

    pending :


    Returns
    -------

    """

    ftp = FTP(ftp_url)  # connect to host, default port
    ftp.login()  # user anonymous, passwd anonymous@
    ftp.cwd("/pub/pdb/derived_data/index")

    if not outdir:
        if "HOME" in os.environ:
            outdir = os.environ["HOME"]
        elif "HOMEPATH" in os.environ:
            outdir = os.environ["HOMEPATH"]

    source_outfile = os.path.join(outdir, "FTP_source.idx")
    ftp.retrbinary("RETR source.idx", open(source_outfile, "wb").write)

    if pending:
        pending_outfile = os.path.join(outdir, "FTP_pending.list")
        ftp.retrbinary("RETR pending_waiting.list", open(pending_outfile, "wb").write)

    ftp.quit()

    pass


def preprocess_mentions(mentions, verbose=False):
    """

    Parameters
    ----------
    mentions :

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    """
    unpacked = []

    for i in list(mentions):
        # decomposed = decompose_slashed_residue(i)
        decomposed = subdivide_compound_residues(i)
        if verbose:
            print(decomposed)

        for j in decomposed:
            if j not in unpacked:
                unpacked.append(j)
            else:
                mentions.remove(j)

    return mentions


def _locate_residues(fulltext_tokens, residue_mentions, offset=False, verbose=False):
    """
    Locate the already identified residues within tokenised fulltext.

    Parameters
    ----------
    fulltext_tokens : List of strings
                  Array of tokens to search for *residue_mentions*, typically
                  the output from :func:`~pyresid.reconstruct_fulltext`.

    residue_mentions : Set, or other iterable.
                   Typically output from :func:`~pyresid._identify_residues`,
                   the residues which to locate within *fulltext_tokens*.

    offset : Bool, optional, default: False
         Flag to include the location index (idx) within the *location_dict*,
         value is the number of characters the start of the matched string is
         from the start of the fulltext document from which *fulltext_tokens*
         was tokenised.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    location_dict : OrderedDict
                Ordered dictionary containing the locations, matched string
                and frequency of each of the residue mentions passed, which
                are themselves used as the dict keys.

    See Also
    --------
    :func:`~pyresid._identify_residues`

    """

    ## preprocess mentions
    residue_mentions = preprocess_mentions(residue_mentions)

    if verbose:
        print(residue_mentions)

    location_dict = OrderedDict()

    for i, tkn in enumerate(residue_mentions):

        # locations = [word for word in enumerate(fulltext_tokens) if tkn in word[1]]
        locations = [word for word in enumerate(fulltext_tokens) if
                     tkn in str(word[1])]  ## Should make it possible to pass spacy tokens
        if verbose:
            print(i, " tkn=", tkn)
            print(locations, len(locations))

        # decomposed = decompose_slashed_residue(tkn)
        decomposed = subdivide_compound_residues(tkn)

        for newtkn in decomposed:

            where = [newtkn in _identify_residues(str(word[1]), verbose=False) for word in enumerate(fulltext_tokens)]

            if verbose:
                print("newtkn=", newtkn)
                print("locations = ", np.arange(len(fulltext_tokens))[where])
                print("Strings = ", np.array(fulltext_tokens)[where])

            if newtkn not in location_dict:
                #                 print("unknown")
                location_dict[newtkn] = OrderedDict()
                if offset:
                    location_dict[newtkn]["offset"] = []
                location_dict[newtkn]["locations"] = []
                location_dict[newtkn]["string"] = []
                location_dict[newtkn]["freq"] = 0
            else:
                #                 print("known")
                pass

            location_dict[newtkn]["locations"] = np.arange(len(fulltext_tokens))[where]
            location_dict[newtkn]["string"] = np.array(fulltext_tokens)[where]
            location_dict[newtkn]["freq"] = len(location_dict[newtkn]["locations"])

            if offset:
                location_dict[newtkn]["offset"] = [word.idx for word in location_dict[newtkn]["string"]]

    return location_dict


def _deprecated_locate_residues(fulltext_tokens, unique_residues, fulldoc=None, offset=False, verbose=False):
    """
    DEPRECATED

    Parameters
    ----------
    fulltext_tokens :

    unique_residues :

    fulldoc :

    offset :

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    """
    location_dict = OrderedDict()

    for tkn in unique_residues:
        if verbose:
            print(tkn)
        # decomposed = decompose_slashed_residue(tkn)
        decomposed = subdivide_compound_residues(tkn)

        for newtkn in decomposed:
            if newtkn not in location_dict:
                location_dict[newtkn] = OrderedDict()
                if offset:
                    location_dict[newtkn]["offset"] = []
                location_dict[newtkn]["locations"] = []
                location_dict[newtkn]["string"] = []
                location_dict[newtkn]["freq"] = 0

            if verbose:
                print(newtkn)

            # locations = [word for word in enumerate(fulltext_tokens) if tkn in word[1]]
            locations = [word for word in enumerate(fulltext_tokens) if
                         tkn in str(word[1])]  ## Should make it possible to pass spacy tokens
            if offset:
                location_dict[newtkn]["offset"] += [word[1].idx for word in locations]

            location_dict[newtkn]["locations"] += list(np.array(locations).T[0].astype(int))
            location_dict[newtkn]["string"] += list(np.array(locations).T[1])
            location_dict[newtkn]["freq"] += len(location_dict[newtkn]["locations"])

            if verbose:
                print(locations)

    return location_dict


def plot_locations(location_dict, text_dict, fulltext_tokens, n=None, title=None, ylabel=None):
    """

    Parameters
    ----------
    location_dict :

    text_dict :

    fulltext_tokens :

    n :

    title :

    ylabel :


    Returns
    -------

    """

    keys_freqency_sorted = np.array(list(location_dict.keys()))[
                               np.argsort([location_dict[res]["freq"] for res in location_dict])][::-1]

    if n:
        if n > len(keys_freqency_sorted):
            n = len(keys_freqency_sorted)
        plot_dict = extract(keys_freqency_sorted[:n], location_dict)
        keys_freqency_sorted = keys_freqency_sorted[:n]
    else:
        plot_dict = location_dict

    fig = plt.figure(figsize=[8, len(plot_dict.keys()) * 0.5])
    fig.subplots_adjust(left=0.15, bottom=0.08, top=0.99,
                        right=0.99, hspace=0, wspace=0)

    ax1 = fig.add_subplot(111)

    for i, key in enumerate(keys_freqency_sorted[::-1]):  # Frequency sorted
        ax1.scatter(plot_dict[key]["locations"], np.ones(len(plot_dict[key]["locations"])) * i, marker="|")

    orig_ylim = ax1.get_ylim()

    for sec in text_dict:
        ax1.plot([text_dict[sec]["start"], text_dict[sec]["start"], ],
                 [np.nanmin(orig_ylim) - 2, np.nanmax(orig_ylim) + 2])  # Introduction
        ax1.text(text_dict[sec]["start"], orig_ylim[1] - 1, text_dict[sec]["title"], rotation="vertical", ha="left")

    plt.yticks(np.arange(len(plot_dict)), keys_freqency_sorted[::-1])

    if n:
        ax1.set_ylim([-0.5, n - 0.5])
    else:
        ax1.set_ylim(orig_ylim)

    if title:
        ax1.set_title(title)

    ax1.set_xlim(-100, len(fulltext_tokens))
    plt.xlabel("Token Number")

    if ylabel:
        plt.ylabel(ylabel)
    else:
        plt.ylabel("Amino Acid")

    pass


def find_clusters(residue_shortid, locations_dict):
    """

    Parameters
    ----------
    residue_shortid
    locations_dict

    Returns
    -------

    """
    X = np.array([locations_dict[residue_shortid]["locations"],
                  list(np.array(
                      locations_dict[residue_shortid]["locations"]) * 0 + 1)]).T  # Assume all are at the same y

    # generate the linkage matrix
    # Z = linkage(X, 'ward')
    # Z = linkage(X, 'single', metric="cosine")
    # Z = linkage(X, 'complete', metric="cosine")
    Z = linkage(X, 'average', metric="cosine")

    return Z


def plot_dendrogram(Z, figsize=[10, 10], orientation="left", leaf_rotation=90., max_d=None,
                    title='Hierarchical Clustering Dendrogram'):
    """

    Parameters
    ----------
    Z :

    figsize :

    orientation :

    leaf_rotation :

    max_d :

    title :


    Returns
    -------

    """
    fig = plt.figure(figsize=figsize)
    fig.subplots_adjust(left=0.15, bottom=0.08, top=0.99,
                        right=0.99, hspace=0, wspace=0)

    ax1 = fig.add_subplot(111)

    # ax1.set_title(r'$\textnormal{Hierarchical Clustering Dendrogram}$')
    # ax1.set_xlabel(r'$\textnormal{sample index}$')
    # ax1.set_ylabel(r'$\textnormal{distance}$')

    if title:
        ax1.set_title(title)
    if orientation == "left" or orientation == "right":
        ax1.set_ylabel('sample index')
        ax1.set_xlabel('distance')
        if orientation == "left":
            ax1.yaxis.set_label_position("right")
    else:
        ax1.set_xlabel('sample index')
        ax1.set_ylabel('distance')

    dendrogram(
        Z,
        orientation=orientation,
        leaf_rotation=leaf_rotation,  # rotates the x axis labels
        leaf_font_size=8.,  # font size for the x axis labels
        ax=ax1
    )

    if max_d:
        plt.axhline(y=max_d, c='k')

    plt.show()
    pass


def plot_disthist(Z, bins=None, max_d=None):
    """

    Parameters
    ----------
    Z :

    bins :

    max_d :


    Returns
    -------

    """
    if not bins:
        binsize = np.nanmax(Z.T[2]) / 20.
        bins = np.linspace(0, np.nanmax(Z.T[2]) + binsize, 100)

    fig = plt.figure(figsize=[8, 4])
    fig.subplots_adjust(left=0.15, bottom=0.08, top=0.99,
                        right=0.99, hspace=0, wspace=0)

    ax1 = fig.add_subplot(111)

    #     hist = ax1.hist(Z.T[2], bins=bins, cumulative=True)
    hist = ax1.hist(Z.T[2], bins=bins)

    if max_d:
        plt.axvline(x=max_d, c="k")

    ax1.set_title("Cluster Distance Histogram")
    ax1.set_xlabel("Distance")
    ax1.set_ylabel("Frequency")
    pass


def plot_clusters(X, Z, max_d, text_dict, fulltext_tokens, residue_shortid):
    """

    Parameters
    ----------
    X :

    Z :

    max_d :

    text_dict :

    fulltext_tokens :

    residue_shortid :


    Returns
    -------

    """

    clusters = fcluster(Z, max_d, criterion='distance')

    fig = plt.figure(figsize=[8, 1])
    fig.subplots_adjust(left=0.15, bottom=0.08, top=0.99,
                        right=0.99, hspace=0, wspace=0)

    ax1 = fig.add_subplot(111)

    ax1.scatter(X[:, 0], X[:, 1], c=clusters, cmap='plasma', marker="|", s=100)

    orig_ylim = ax1.get_ylim()

    ax1.plot([text_dict["Sec1"]["start"], text_dict["Sec1"]["start"], ],
             [np.nanmin(orig_ylim) - 2, np.nanmax(orig_ylim) + 2])  # Introduction
    ax1.text(text_dict["Sec1"]["start"], orig_ylim[1], "Introduction", rotation="vertical", ha="left")
    ax1.plot([text_dict["Sec2"]["start"], text_dict["Sec2"]["start"], ],
             [np.nanmin(orig_ylim) - 2, np.nanmax(orig_ylim) + 2])  # Results
    ax1.text(text_dict["Sec2"]["start"], orig_ylim[1], "Results", rotation="vertical", ha="left")
    ax1.plot([text_dict["Sec12"]["start"], text_dict["Sec12"]["start"], ],
             [np.nanmin(orig_ylim) - 2, np.nanmax(orig_ylim) + 2])  # Discussion
    ax1.text(text_dict["Sec12"]["start"], orig_ylim[1], "Discussion", rotation="vertical", ha="left")

    plt.yticks([1, ], [residue_shortid], )

    ax1.set_ylim(orig_ylim)

    ax1.set_xlim(-100, len(fulltext_tokens))
    plt.xlabel("Token Number")
    plt.ylabel("Amino Acid")
    pass


def query_epmc(query):
    """

    Parameters
    ----------
    query :

    Returns
    -------

    """
    url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search?query="
    page_term = "&pageSize=999"  ## Usual limit is 25
    request_url = url + query + page_term
    r = requests.get(request_url)

    if r.status_code == 200:
        return r
    else:
        warnings.warn("request to " + str(query) + " has failed to return 200, and has returned " + str(r.status_code))
    pass


def query_to_dict(query):
    """

    Parameters
    ----------
    query :

    Returns
    -------

    """
    r = query_epmc(query)
    soup = BeautifulSoup(r.text, "lxml-xml")

    results_dict = OrderedDict()

    for result in soup.resultList:
        #     print(result.title)
        results_dict[result.id] = {}

        for i in result:
            results_dict[result.id][i.name] = i.text

    return results_dict


def extract_ids_from_query(query):
    """

    Parameters
    ----------
    query :

    Returns
    -------

    """

    results_dict = query_to_dict(query)

    ids = [results_dict[i]["pmcid"] for i in results_dict]

    return ids


def get_rawcontext(location_dict, fulltext, x=50, verbose=False):
    """

    Parameters
    ----------
    location_dict : OrderedDict
                Ordered dictionary containing the locations, matched string
                and frequency of each of the residue mentions passed, which
                are themselves used as the dict keys.

    fulltext :  string
                text snippet to be subdivide for context.

    x : int, optional, default: 50
        How much context to include

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    location_dict : OrderedDict
                Returns an augmented version of the input `location_dict`, with the context
                added as an item under the key "rawcontext" for each unique mention and location
    """
    for key in location_dict:
        if verbose:
            print(key)
        for i, offset in enumerate(location_dict[key]["offset"]):
            if i == 0:
                location_dict[key]["rawcontext"] = []

            #             rawcontext = "".join(fulltext[offset-x:offset+x])
            rawcontext = [fulltext[offset - x:offset + x], ]

            location_dict[key]["rawcontext"] += rawcontext

            if verbose:
                print(rawcontext)
    return location_dict


def _process(ext_id_list, outdir, filename="pyresid_output.json", save=True, return_dict=False, cifscantype='flex',
            context="sent", use_actual_sections=False, verbose=False):
    """
    This wraps the main workhorse functions, taking a list of PMC IDs and mining the resulting fulltext.
    output is a json structure (the encoded output of _locate_residues2 saved with MyEncoder), to match
    the EBI specifications.

    Parameters
    ----------
    ext_id_list : List of strings
         List containing ePMC identifiers used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    outdir : String or Path
         Directory that will contain the output file.

    filename : String
         The structured output JSON file containing the annotations, one line per `ext_id`.

    save : Bool, optional, default: True
       Flag to turn off writing the  JSON. Good for debugging whenm combined with `return_dict`.

    return_dict : Bool, optional, default: False
              Flag to return the output as a dictionary

    cifscantype : {"flex", "standard"}, default: "flex"
              Flag passed to `pycifrw` via :func:`~pyresid._locate_residues2`; `scantype` can be `standard` or `flex`.  `standard` provides
              pure Python parsing at the cost of a factor of 10 or so in speed.  `flex` will tokenise
              the input CIF file using fast C routines.  Note that running PyCIFRW in Jython uses
              native Java regular expressions to provide a speedup regardless of this argument.

    context : String, optional, default: "sent"
          Flag passed to :func:`~pyresid._locate_residues2` to determine the type of context added
          to annotations, "sent" uses the spaCy parsed sentences, anything else will use `x`
          characters either side of the matched tokens.

    use_actual_sections : Bool, optional, default: False
                      Flag passed to :func:`~pyresid._locate_residues2`, in order to use the actual
                      titles rather than the fuzzy matches to the EBI section whitelist.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    (optional) outdict: OrderedDict
         Dictionary containing the annotations. Good for debugging.

    See Also
    --------
    * :func:`~pyresid._locate_residues2`
    """

    nlp = spacy.load('en')
    outpath = os.path.join(os.path.abspath(outdir), filename)

    if isinstance(ext_id_list, str):  ## Check that the passed list is valid (if single string, parses as single ID)
        ext_id_list = [ext_id_list, ]

    outdict = {}

    for ext_id in ext_id_list:
        print("Processing", ext_id)

        text_dict = get_text(ext_id)                                ##retrieve the text
        fulltext = reconstruct_fulltext(text_dict, tokenise=False)  ## convert from sections to fulltext
        fulldoc = nlp(fulltext)                                     ## load into a spaCy doc for tokenising
        fulltext_tokens = [t for t in fulldoc]                      ## get list of tokens

        residue_mentions = _identify_residues(fulltext, return_mentions=True)    ## Find residue mentions
        unique_proteins = identify_protein_ID(fulltext)                         ## find PDB structure mentions

        output_dict = _locate_residues2(fulltext_tokens=fulltext_tokens, residue_mentions=residue_mentions,
                                        unique_proteins=unique_proteins,
                                        text_dict=text_dict, fulltext=fulltext, ext_id=ext_id,
                                        cifscantype=cifscantype, use_actual_sections=use_actual_sections,
                                        context=context, verbose=verbose)
        if save:
            with open(outpath, 'a') as outfile:
                json.dump(output_dict, outfile, cls=MyEncoder, separators=(',', ':'))
                outfile.write("\n")
        outdict[ext_id] = output_dict
        if verbose:
            print(output_dict)

    if return_dict:
        return outdict
    else:
        pass
    pass


def _locate_residues2(fulltext_tokens, residue_mentions, text_dict, ext_id, fulltext,
                      unique_proteins, fulldoc=False, provider="West-Life", offset=True, x=50,
                      include_sentence=False, include_token=False, cifscantype="flex",
                      context="sent", use_actual_sections=False, verbose=False, ):
    """

    Locate the already identified residues within tokenised fulltext.

    Parameters
    ----------
    fulltext_tokens : List of strings
                  Array of tokens to search for *residue_mentions*, typically
                  the output from :func:`~pyresid.reconstruct_fulltext`.

    residue_mentions : Set, or other iterable.
                   Typically output from :func:`~pyresid._identify_residues`,
                   the residues which to locate within *fulltext_tokens*.

    text_dict : Dict or OrderedDict
            Dictionary containing the parsed XML. Each entry corresponds to a Section
            in the XML. Usually an output from :func:`~pyresid.get_sections_text`

    ext_id : String
         ePMC identifier used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    fulltext :  string
                text snippet to be searched for residues.

    unique_proteins : Set, or other iterable.
                  PDB entry matches found within `fulltext` - usually an output from :func:`~pyresid.identify_protein_ID`.
    fulldoc : optional, default: False
          If a spaCy `fulldoc` is not passed, one is created using `fulltext`.

    provider : String, default: "West-Life".
           The string used to link back to the annotation proveneance. Stipulation of the EBI.

    offset : Bool, optional, default: False
         Flag to include the location index (idx) within the `location_dict`,
         value is the number of characters the start of the matched string is
         from the start of the fulltext document from which `fulltext_tokens`
         was tokenised.

    x : int, optional, default: 50
        How much context to include if context is set to something other than "sent"

    include_sentence : Bool, optional, default: False
                   Flag to set the context behaviour - will return a spaCy sentence rather than
                   raw context.

    include_token : Bool, optional, default: False
                Flag to determine whether to return the matched token in the `annsdict`

    cifscantype : {"flex", "standard"}, default: "flex"
              Flag passed to `pycifrw`; `scantype` can be `standard` or `flex`.  `standard` provides
              pure Python parsing at the cost of a factor of 10 or so in speed.  `flex` will tokenise
              the input CIF file using fast C routines.  Note that running PyCIFRW in Jython uses
              native Java regular expressions to provide a speedup regardless of this argument.

    context : String, optional, default: "sent"
          Flag passed to determine the type of context added to annotations, "sent" uses the spaCy
          parsed sentences, anything else will use *x* characters either side of the matched tokens.

    use_actual_sections : Bool, optional, default: False
                      Flag passed to use the actual titles rather than the fuzzy matches to the EBI
                      section whitelist.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output


    Returns
    -------

    location_dict : OrderedDict
                Ordered dictionary containing the locations, matched string
                and frequency of each of the residue mentions passed, which
                are themselves used as the dict keys.

    See Also
    --------
    * :func:`~pyresid._identify_residues`
    * :func:`~pyresid._locate_residues`
    * `pyresid.EBI_whitelist`: list of allowed section titles for the EBI

    """

    sec_names = [text_dict[i]["title"] for i in text_dict]
    sec_offset = [text_dict[i]["offset"] for i in text_dict]

    ## preprocess mentions
    residue_mentions = preprocess_mentions(residue_mentions)

    if verbose:
        print(residue_mentions)

    if not fulldoc:
        nlp = spacy.load('en')
        fulldoc = nlp(fulltext)

    location_dict = OrderedDict()

    for i, tkn in enumerate(residue_mentions):

        ## Locate the residue within the list of tokens
        locations = [word for word in enumerate(fulltext_tokens) if
                     tkn in str(word[1])]  ## Should make it possible to pass spacy tokens
        if verbose:
            print(i, " tkn=", tkn)
            print(locations, len(locations))

        ## convert exact mentions into individual residual - this deals with hyphens and slashes
        decomposed = subdivide_compound_residues(tkn)

        ## Loop through the decomposed tokens - exact mention is the same, but the individual residue is distinct
        for newtkn in decomposed:


            where = [newtkn in _identify_residues(str(word[1]), verbose=False) for word in enumerate(fulltext_tokens)]

            if verbose:
                print("newtkn=", newtkn)
                print("locations = ", np.arange(len(fulltext_tokens))[where])
                print("Strings = ", np.array(fulltext_tokens)[where])

            if newtkn not in location_dict:
                # if no key for token, make a new datastructure
                location_dict[newtkn] = OrderedDict()
                if offset:
                    location_dict[newtkn]["offset"] = []
                location_dict[newtkn]["locations"] = []
                location_dict[newtkn]["string"] = []
                location_dict[newtkn]["freq"] = 0
            else:
                #                 print("known")
                pass

            location_dict[newtkn]["locations"] = np.arange(len(fulltext_tokens))[where]
            location_dict[newtkn]["string"] = np.array(fulltext_tokens)[where]
            location_dict[newtkn]["freq"] = len(location_dict[newtkn]["locations"])

            if offset:
                location_dict[newtkn]["offset"] = [word.idx for word in location_dict[newtkn]["string"]]

    get_rawcontext(location_dict, fulltext, x=50, verbose=False)

    output_dict = OrderedDict()
    #     output_dict["pmcid"] = ext_id
    output_dict["pmcid"] = "".join(filter(str.isdigit, ext_id))
    output_dict["provider"] = provider
    output_dict["anns"] = []

    ## SETUP STRUCT ASSOCIATE
    accession_numbers = []
    struct_dict = {}

    ## Loop through PDB structures to load in the structures, to check the candidates against
    for prot_structure in unique_proteins:

        if verbose:
            print(prot_structure, end=" ")
        ## Many PDB entries can share a since UniProt accession URI
        accession_id = find_UniProt_accession(prot_structure)
        accession_numbers.append(accession_id)

        # Only do this if one is found!
        if accession_id:
            infile = os.path.join(mmCIF_dir, prot_structure + ".cif")

            # Only download if it is not known locally
            if not os.path.exists(infile):
                download_pdbfile(pdb_id=prot_structure, outfile=infile)

            # Load in the cif file to get the structure
            if cifscantype not in  {"flex", "standard"}:
                cifscantype = "flex"
            cf = load_pdbfile(prot_structure, infile=infile, cifscantype=cifscantype)

            res_seq = get_residue_sequence(cf)
            struct_dict[prot_structure] = res_seq

            if verbose:
                print("")

    unique_accession_numbers = set(accession_numbers)
    found_accession = [i for i in zip(unique_proteins, accession_numbers) if i[1]]

    for residue in location_dict:

        ## ASSOCIATE - MATCH TO STRUCT

        matched = []

        for k in found_accession:

            if verbose:
                print(k[0])

            if residue in struct_dict[k[0]]:
                if verbose:
                    print("match")
                matched.append(k[1])

        ## CHOOSE URI
        if len(matched) == 0:
            matched.append(
                "NO MATCH FOUND")  ## -ASK WHETHER TO DROP UNMATCHED - see issue #2 http://hcp004.hartree.stfc.ac.uk/RobFirth/PDB-protein-res/issues/2
        uniprot_uri = np.unique(matched)[0]  ##Do better than just choosing the first one -
        ## duplicate entry with both URI, use ML - TODO!
        ## End Association
        locations = location_dict[residue]["locations"]
        offsets = location_dict[residue]["offset"]

        for offset, location in zip(offsets, locations):
            annsdict = OrderedDict()

            annsdict["exact"] = fulltext_tokens[location]
            if include_token:
                annsdict["token"] = residue

            if context == "sent":

                for sent in fulldoc.sents:
                    if offset >= sent.start_char and offset < sent.end_char:
                        context_sent = sent

                        if include_sentence:
                            annsdict["sent"] = sent

                if verbose:
                    print(context_sent.text, annsdict["exact"].string)
                start_index = context_sent.text.index(annsdict["exact"].string)
                annsdict["prefix"] = context_sent.text[:start_index]
                annsdict["postfix"] = context_sent.text[start_index+len(annsdict["exact"].string):]

            else:

                ## gather context prefix-postfix
                annsdict["prefix"] = fulltext[offset - x:offset]
                annsdict["postfix"] = fulltext[
                                      offset + len(fulltext_tokens[location]):offset + len(fulltext_tokens[location]) + x]

            ## ID the section
            w = np.digitize(offset, sec_offset) - 1
            annsdict["section"] = sec_names[w]

            if not use_actual_sections:
                fuzzymatch = fwprocess.extract(annsdict["section"], EBI_whitelist, limit=1)[0]
                annsdict["section"] = fuzzymatch[0]

            annsdict["tags"] = []

            tagdict = OrderedDict()
            # tagdict["name"] = location[1]
            #             tagdict["name"] = residue
            tagdict["name"] = uniprot_uri
            tagdict["uri"] = "https://www.uniprot.org/uniprot/" + uniprot_uri

            annsdict["tags"].append(tagdict)

            output_dict["anns"].append(annsdict)

    return output_dict


def get_currentPDBIDs(url="http://www.rcsb.org/pdb/rest/getCurrent"):
    """

    :param url:
    :return:
    """
    r = requests.get(url)

    soup = BeautifulSoup(r.text, "lxml-xml")

    return [i["structureId"] for i in soup.findAll("PDB")]


def download_pdbfile_RCSB(pdb_id, outfile=None, overwrite=True):
    """

    :param pdb_id:
    :param outfile:
    :param overwrite:
    :return:
    """
    # pdb_id = '1LAF'
    # pdb_id = '4P0I'
    # pdb_file = pdb.get_pdb_file(pdb_id, filetype="mmCIF")

    if not outfile:
        outfile = os.path.join(os.path.abspath(os.curdir), pdb_id + ".cif")  ## TODO - needs to point at mmCIF dir

    if os.path.exists(outfile) and not overwrite:
        print("File already exists at", outfile)
        print("run with overwrite=True to force")
        return

    request_url = "https://files.rcsb.org/download/" + pdb_id + ".cif"
    # pdb_file
    r = requests.get(request_url)

    if r.status_code == 200:
        print("saving to", outfile)
        with open(outfile, 'w') as f:
            f.write(r.text)

        pass
    else:
        warnings.warn("request to " + str(pdb_id) + " has failed to return 200, and has returned " + str(r.status_code))
    pass


def download_pdbfile(pdb_id, outfile=None, overwrite=True, verbose=True):
    """

    Parameters
    ----------
    pdb_id :
    outfile :
    overwrite :
    verbose :

    Returns
    -------

    """
    # pdb_id = '1LAF'
    # pdb_id = '4P0I'
    # pdb_file = pdb.get_pdb_file(pdb_id, filetype="mmCIF")

    if not outfile:
        outfile = os.path.join(os.path.abspath(os.curdir), pdb_id + ".cif")  ## TODO - needs to point at mmCIF dir
        if verbose: print("Saving to", outfile)

    if os.path.exists(outfile) and not overwrite:
        print("File already exists at", outfile)
        print("run with overwrite=True to force")
        return

    ## Get via FTP
    ftp_dir = pdb_id[1:3].lower()

    mmcif_path = "pub/databases/pdb/data/structures/divided/mmCIF/" + ftp_dir

    ftp = FTP("ftp.ebi.ac.uk")

    ftp.login()
    ftp.cwd(mmcif_path)

    ftp.retrbinary("RETR " + pdb_id.lower() + ".cif.gz", open(outfile.replace(".cif", ".cif.gz"), 'wb').write)

    ftp.quit()

    ## Decompress
    with gzip.open(open(outfile.replace(".cif", ".cif.gz"), 'rb')) as ifile:
        with open(outfile, "wb") as ofile:
            shutil.copyfileobj(ifile, ofile)

    pass

def load_struct_from_pdbfile(pdb_id, infile=None):
    """
    DEPRECATED
    :param pdb_id:
    :param infile:
    :return:
    """
    p = MMCIFParser()

    if not infile:
        infile = os.path.join(os.path.abspath(os.curdir), pdb_id + ".cif")  ## TODO - needs to point at mmCIF dir

    structure = p.get_structure(pdb_id, infile)

    return structure


def load_pdbfile(pdb_id, infile=None, cifscantype='flex'):
    """

    :param pdb_id:
    :param infile:
    :return:
    """
    if not infile:
        infile = os.path.join(mmCIF_dir, pdb_id + ".cif")

    cf = CifFile.ReadCif(infile, scantype=cifscantype)

    return cf


def get_residue_sequence(cf):
    """
    used to work with load_struct_from_pdbfile, now uses load_pdbfile output
    :param  structure:
    :return:
    """
    # residue_sequence = list(np.array([[residue.resname.capitalize(), int(residue.id[1])] for residue in
    #                                    list(structure.get_chains())[0].get_residues()]).T)

    pdb = cf.keys()[0]
    cf_mon = [s.capitalize() for s in cf[pdb]["_entity_poly_seq.mon_id"]]
    cf_num = cf[pdb]["_entity_poly_seq.num"]

    ## Identify where the chain begins (after the signal peptide)
    try:
        align_beg = int(cf[pdb]["_struct_ref_seq.db_align_beg"])  ## if only one chain
        align_beg = align_beg - 1
    except:
        align_beg = int(cf[pdb]["_struct_ref_seq.db_align_beg"][0])  ## beginning index
        align_beg = align_beg - 1

    cf_num = [str(int(i) + align_beg - 1) for i in cf_num]  ## -1 again ?

    residue_sequence = [x + y for x, y in zip(cf_mon, cf_num)]

    return residue_sequence


def get_PDB_summary(PDB_id, verbose=False):
    """
    Similar info to get_meta, but for a PDB entry - returns info about when it was regitered etc.

    Parameters
    ----------
    PDB_id :

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    See Also
    --------
    *:func:`~pyresid.get_meta`
    """

    status = get_PDB_status(PDB_id=PDB_id, verbose=verbose)

    if status == "CURRENT":
        url = "http://www.rcsb.org/pdb/rest/describePDB?structureId=" + PDB_id
    elif status == "UNRELEASED":
        url = "http://www.rcsb.org//pdb/rest/getUnreleased?structureId=" + PDB_id
    else:
        print("Do not recognise status", status)
        return False

    r = requests.get(url)
    if r.status_code == 200:

        soup = BeautifulSoup(r.text, "lxml-xml")

        if status == "CURRENT":
            attrs_dict = soup.find("PDB").attrs

            if hasattr(soup.find("PDB"), "contents"):
                contents = [i for i in soup.find("PDB").contents if i != "\n"]
                for item in contents:

                    if item.name not in attrs_dict:
                        attrs_dict[item.name] = []
                    if "pdbId" in item.attrs:
                        attrs_dict[item.name].append(item.attrs["pdbId"])
                    else:
                        attrs_dict[item.name].append(item.attrs)

            else:
                if verbose:
                    print("No contents")

        elif status == "UNRELEASED":
            attrs_dict = soup.find("record").attrs

            if hasattr(soup.find("record"), "contents"):

                contents = [i for i in soup.find("record").contents if i != "\n"]

                for item in contents:
                    if verbose:
                        print(item.name)

                    if len(item.attrs) > 0:
                        for key in item.attrs:
                            attrs_dict[key] = item.attrs[key]
                    if len(item.contents) > 0:
                        attrs_dict[item.name] = item.contents

            else:
                if verbose:
                    print("No contents")

        return attrs_dict

    else:
        warnings.warn("request to " + str(PDB_id) + " has failed to return 200, and has returned " + str(r.status_code))
    pass


def get_PDB_status(PDB_id, verbose=False):
    """
    Check status (released/unreleased etc.) of PDB

    Parameters
    ----------
    PDB_id :

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------

    """

    # PDB_id = "4POW"
    url = "http://www.rcsb.org/pdb/rest/idStatus?structureId=" + PDB_id
    r = requests.get(url)
    soup = BeautifulSoup(r.text, "lxml-xml")

    status = soup.record["status"]

    if verbose:
        print(status)

    return status


def get_annotations(ext_id):
    """

    Parameters
    ----------
    ext_id

    Returns
    -------

    """
    url = "https://www.ebi.ac.uk/europepmc/annotations_api/annotationsByArticleIds?articleIds="
    request_url = url + "PMC:" + ext_id
    r = requests.get(request_url)
    annotations = json.loads(r.text)[0]

    return annotations


def get_accession_numbers(annotations=None, ext_id=None):
    """

    Parameters
    ----------
    annotations
    ext_id

    Returns
    -------

    """
    if not annotations:
        annotations = get_annotations(ext_id)

    pdb_ids = list(set([anno["exact"] for anno in annotations["annotations"] if anno["type"] == "Accession Numbers"]))
    return pdb_ids


def find_UniProt_accession(pdb_id, verbose=False):
    """
    Note - returns the first hit acc num, which should be the one that corresponds to the "recommended name"

    :param pdb_id:
    :param verbose:
    :return:
    """

    request_url = r"https://www.uniprot.org/uniprot/?query=database:(type:pdb " + pdb_id + ")&format=xml"

    r = requests.get(request_url)
    soup = BeautifulSoup(r.text, "lxml-xml")

    ## TODO - Multiple results?
    if soup.find("accession"):
        first_hit_URI = soup.find("accession").text
    else:
        print("accession not found for", pdb_id)
        first_hit_URI = False

    if verbose:
        print(first_hit_URI)

    return (first_hit_URI)


def check_loc_dict(loc_dict):
    for res in loc_dict:
        print(res)
        freq = loc_dict[res]["freq"]
        n_offset = len(loc_dict[res]["offset"])
        n_locations = len(loc_dict[res]["locations"])
        print(freq, n_offset, n_locations)


def load_nagel_corpus_text(identifier, nagel_path="/Users/berto/projects/pdbres/data/NagelCorpus/NagelCorpusText/"):
    """

    Parameters
    ----------
    identifier
    nagel_path

    Returns
    -------

    """

    infile = os.path.join(nagel_path, identifier + ".txt")

    with open(infile, "r") as ifile:
        fulltext = ifile.read().replace("\n", "")

    source = SourceClass()

    source.fulltext = fulltext
    nlp = spacy.load('en')
    source.doc = nlp(fulltext)

    return source


def _identify_residues(fulltext, verbose=False):
    """
    Uses Regular Expressions to identify and locate usages of residues within the supplied `fulltext`. Returns a
    list of MatchClass objects that contain the start and end of the match within the text, and the matched string.
    For compound matches, a list of positions and residues is included in the match, which needs decomposing before
    further use.

    Parameters
    ----------
    fulltext :  string
            text to be searched for residues.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    matches : List of :class:`~pyresid.MatchClass` objects
        The matches found within `fulltext`.

    See Also
    --------
    * :func:`~pyresid.locate_residues`
    * :func:`~pyresid.process`
    """
    ## Residue Position
    # pattern_POS = "((\d+[\),.\s'\"/])|(\d+\Z))"
    pattern_POS = "(\d+[\),.\s'\"/])|(\d+\Z)"
    # pattern_POS_slash = "(\d+/)+\d+[\),.\s'\"]"
    pattern_POS_slash = "(\d+/)+((\d+[\),.\s'\"/])|(\d+\Z))"

    ## Residue Name-Full
    pattern_RESNF = r"[aA]lanine|[aA]rginine|[aA]sparagine|[aA]spartate|[aA]spartateic [aA]cid|[cC]ysteine|[gG]lutamine|[gG]lutamate|[gG]lutamic [aA]cid|[gG]lycine|[hH]istidine|[iI]soleucine|[lL]eucine|[lL]ysine|[mM]ethionine|[pP]henylalanine|[pP]roline|[sS]erine|[tT]hreonine|[tT]ryptophan|[tT]yrosine|[vV]aline|[pP]yrrolysine|[sS]elenocysteine|[aA]spartic [aA]cid|[aA]sparagine|[gG]lutamic [aA]cid|[gG]lutamine"
    ## Residue Name-1 Letter
    pattern_RESN1 = "[ARNDCQEGHILKMFPSTWYVOUBZX]"
    ## Residue Name-3 Letter
    # pattern_RESN3 = "[aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA"
    pattern_RESN3 = "[aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa"
    ## Residue Name-3 Letter repeating dashes
    pattern_RESN3dr = "(([,\s'\"]((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))|((\(((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))"
    ## Residue Name-3 Letter (with brackets)
    pattern_RESN3b = "([aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa)(\(\d+\))"
    ##
    pattern_standard_join_slash = "((" + pattern_RESN3 + ")(\d+\/))+((" + pattern_RESN3 + ")(" + pattern_POS + "))"

    pattern_standard = "(" + pattern_RESNF + ") " + pattern_POS + "|((" + pattern_RESN3 + ")( " + pattern_POS + \
                       "))|(" + pattern_RESN3 + ")(" + pattern_POS + ")|(,(" + pattern_RESN3 + ")" + pattern_POS + \
                       ")|(" + pattern_RESN3b + ")"
    pattern_standard_slash = "(" + pattern_RESNF + ") " + pattern_POS_slash + "|(" + pattern_RESN3 + ") " + pattern_POS_slash + \
                             "|(" + pattern_RESN3 + ")" + pattern_POS_slash + "|(" + pattern_RESN3b + ")"
    pattern_standard_dash = "(" + pattern_RESN3 + ")-" + pattern_POS

    pattern_site = "(" + pattern_standard + ") residue?" + \
                   "|((" + pattern_RESNF + "|" + pattern_RESN3 + ")(\s|,\s)in\sposition\s\d+)" + \
                   "|((" + pattern_RESNF + "|" + pattern_RESN3 + ")\sresidue(\s|,\s)at\sposition\s\d+)" + \
                   "|(" + pattern_RESNF + "|" + pattern_RESN3 + ")" + "? at position? " + pattern_POS + \
                   "|((" + pattern_RESNF + "|" + pattern_RESN3 + ") residue at position " + pattern_POS + ")" + \
                   "|(residue at position " + pattern_POS + ")" + \
                   "|(((" + pattern_RESN3 + ")|(" + pattern_RESNF + "))\sresidues(\s|,\s)at\spositions\s\d+ and \d+)" + \
                   "|(((" + pattern_RESN3 + ")|(" + pattern_RESNF + "))\sresidues(\s|,\s)at\spositions\s(\d+,)+\d+ and \d+)"


    pattern = "(" + pattern_RESN3dr + ")" + \
              "|(" + pattern_standard_slash + ")" + \
              "|(" + pattern_standard_join_slash + ")" + \
              "|(" + pattern_standard + ")" + \
              "|(" + pattern_standard_dash + ")" + \
              "|(" + pattern_site + ")"

    p = re.compile(pattern)

    ##Tests
    matches = []

    for match in p.finditer(fulltext):

        if verbose:
            print(match.start(), match.end(), match.group())

        m = MatchClass(match.start(), match.end(), match.group())
        m.find_position()
        m.find_amino_acid()
        matches.append(m)

    return matches


def identify_residues(fulltext, verbose=False):
    """
    Uses Regular Expressions to identify and locate usages of residues within the supplied `fulltext`. Returns a
    list of MatchClass objects that contain the start and end of the match within the text, and the matched string.
    For compound matches, a list of positions and residues is included in the match, which needs decomposing before
    further use.
    Based on Ravikumar+ 2012.

    Parameters
    ----------
    fulltext :  string
            text to be searched for residues.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    matches : List of :class:`~pyresid.MatchClass` objects
        The matches found within `fulltext`.

    See Also
    --------
    * :func:`~pyresid.locate_residues`
    * :func:`~pyresid.process`
    """
    ## Amino Acids
    ## Residue Name-Full
    pattern_RESNF = "([aA]lanine|[aA]rginine|[aA]sparagine|[aA]spartate|[aA]spartateic [aA]cid|[cC]ysteine|[gG]lutamine|[gG]lutamate|[gG]lutamic [aA]cid|[gG]lycine|[hH]istidine|[iI]soleucine|[lL]eucine|[lL]ysine|[mM]ethionine|[pP]henylalanine|[pP]roline|[sS]erine|[tT]hreonine|[tT]ryptophan|[tT]yrosine|[vV]aline|[pP]yrrolysine|[sS]elenocysteine|[aA]spartic [aA]cid|[aA]sparagine|[gG]lutamic [aA]cid|[gG]lutamine)"
    ## Residue Name-1 Letter
    pattern_RESN1 = "[ARNDCQEGHILKMFPSTWYVOUBZX]"
    ## Residue Name-3 Letter
    pattern_RESN3 = "([aA]la|[aA]rg|[aA]sn|[aA]sp|[cC]ys|[gG]ln|[gG]lu|[gG]ly|[hH]is|[iI]le|[lL]eu|[lL]ys|[mM]et|[pP]he|[pP]ro|[sS]er|[tT]hr|[tT]rp|[tT]yr|[vV]al|[pP]yl|[sS]ec|[aA]sx|[gG]lx|[xX]aa)"

    #     pattern_WTRES = "("+pattern_RESNF+"|"+pattern_RESN3+"|"+pattern_RESN1+")"
    pattern_WTRES = "(" + pattern_RESNF + "|" + pattern_RESN3 + ")"
    ## Positions
    #     pattern_POS = "(((\d+[\),.\s'\"])|(\d+\Z)))|((\(\d+[\),.\s'\"])|(\d+\)\Z))"
    ## Line 1: Standard Positions, Line 2: Positions with punctuation etc. Line 3: Positions in parentheses, Line 4: Positions preceded by a -
    pattern_POS = "((\d+)" + \
                  "|(((\d+[\),.\s'\"])|(\d+\Z)))" + \
                  "|((\(\d+[\),.\s'\"])|(\d+\)\Z))" + \
                  "|(((-\d+)|(-\d+[\),.\s'\"])|(-\d+\Z)))" + \
                  ")"

    pattern_simple = "(" + pattern_WTRES + pattern_POS + ")|((\Z)" + pattern_WTRES + pattern_POS + ")"

    ## Complex Patterns
    ## Residues - three letter + position repeating with dashes
    pattern_RESN3_dash_repeat = "((([,\s\'\"\(])|(\A))((" + pattern_RESN3 + ")(-\d+|\d+)-)+)((" + pattern_RESN3 + ")(-\d+|\d+))"
    #     pattern_RESN3_dash_repeat = "((([,\s\'\"\(])|(\A))((" + pattern_RESN3 + ")-\d+-)+((" + pattern_RESN3 + ")\d+))"

    ## Slashed Residues
    ## Single Residues, repeating positions, normal or bracketed
    pattern_slashed_pos = "(" + pattern_WTRES + "((-\d+|\(\d+\)|\d+)/)+((-\d+|\(\d+\)|\d+)))"

    ## Repeating Residues and Positions
    pattern_slashed = "(" + pattern_WTRES + "(-\d+|\(\d+\)|\d+)/)+" + "(" + pattern_WTRES + "(-\d+|\(\d+\)|\d+))"

    ## Grammatical Rules
    # pattern_site = "((([,\s\'\"\(])|(\A))" + pattern_WTRES + "(\s|,\s)in\sposition\s\d+)" + \
    #                "|(((([,\s\'\"\(])|(\A))" + pattern_WTRES + ")\sresidues(\s|,\s)at\spositions\s(\d+,)+\d+ and \d+)" + \
    #                "|(((([,\s\'\"\(])|(\A))" + pattern_WTRES + ")\sresidues(\s|,\s)at\spositions\s\d+ and \d+)" + \
    #                "|(((([,\s\'\"\(])|(\A))" + pattern_WTRES + ")\sresidue(\s|,\s)at\sposition\s\d+)" + \
    #                "|(((([,\s\'\"\(])|(\A))" + pattern_WTRES + ") residue at position " + pattern_POS + ")" + \
    #                "|((([,\s\'\"\(])|(\A))" + pattern_WTRES + ") residue?" + \
    #                "|((([,\s\'\"\(])|(\A))" + pattern_WTRES + ")" + "? at position? " + pattern_POS + \
    #                "|(residue(\s|,\s)at\sposition\s\d+)"
    pattern_site = "(" + pattern_WTRES + "(\s|,\s)in\sposition\s\d+)" + \
                   "|((" + pattern_WTRES + ")\sresidues(\s|,\s)at\spositions\s(\d+,)+\d+ and \d+)" + \
                   "|((" + pattern_WTRES + ")\sresidues(\s|,\s)at\spositions\s\d+ and \d+)" + \
                   "|((" + pattern_WTRES + ")\sresidue(\s|,\s)at\sposition\s\d+)" + \
                   "|((" + pattern_WTRES + ") residue at position " + pattern_POS + ")" + \
                   "|(" + pattern_WTRES + ") residue?" + \
                   "|(" + pattern_WTRES + ")" + "? at position? " + pattern_POS + \
                   "|(residue(\s|,\s)at\sposition\s\d+)"

    pattern = "(" + pattern_RESN3_dash_repeat + ")" + \
              "|(" + pattern_site + ")" + \
              "|(" + pattern_slashed_pos + ")" + \
              "|(" + pattern_slashed + ")" + \
              "|(" + pattern_simple + ")"

    p = re.compile(pattern)

    ##Tests
    matches = []

    for match in p.finditer(fulltext):

        if verbose:
            print(match.start(), match.end(), match.group())

        m = MatchClass(match.start(), match.end(), match.group())
        m.find_position()
        m.find_amino_acid()
        matches.append(m)

    return matches

identify_residues_ravikumar = identify_residues

def _identify_residues_nagel(fulltext, verbose=False):
    """

    Parameters
    ----------
    fulltext
    verbose

    Returns
    -------

    """
    ## Residue Position
    pattern_POS = "\d+[\),.\s]"
    pattern_POS_slash = "(\d+/)+\d+"
    ## Residue Name-Full
    pattern_RESNF = r"[aA]lanine|[aA]rginine|[aA]sparagine|[aA]spartate|[aA]spartateic [aA]cid|[cC]ysteine|[gG]lutamine|[gG]lutamate|[gG]lutamic [aA]cid|[gG]lycine|[hH]istidine|[iI]soleucine|[lL]eucine|[lL]ysine|[mM]ethionine|[pP]henylalanine|[pP]roline|[sS]erine|[tT]hreonine|[tT]ryptophan|[tT]yrosine|[vV]aline|[pP]yrrolysine|[sS]elenocysteine|[aA]spartic [aA]cid|[aA]sparagine|[gG]lutamic [aA]cid|[gG]lutamine"
    ## Residue Name-1 Letter
    pattern_RESN1 = "[ARNDCQEGHILKMFPSTWYVOUBZX]"
    ## Residue Name-3 Letter
    pattern_RESN3 = "[aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA"
    ## Residue Name-3 Letter repeating dashes
    pattern_RESN3dr = "(([,\s'\"]((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))|((\(((" + pattern_RESN3 + ")\d+-)+((" + pattern_RESN3 + ")\d+)))"
    ## Residue Name-3 Letter (with brackets)
    pattern_RESN3b = "([aA]la|ALA|[aA]rg|ARG|[aA]sn|ASN|[aA]sp|ASP|[cC]ys|CYS|[gG]ln|GLN|[gG]lu|GLU|[gG]ly|GLY|[hH]is|HIS|[iI]le|ILE|[lL]eu|LEU|[lL]ys|LYS|[mM]et|MET|[pP]he|PHE|[pP]ro|PRO|[sS]er|SER|[tT]hr|THR|[tT]rp|TRP|[tT]yr|TYR|[vV]al|VAL|[pP]yl|PYL|[sS]ec|SEC|[aA]sx|ASX|[gG]lx|GLX|[xX]aa|XAA)(\(\d+\))"

    #     pattern_standard = "(" + pattern_RESNF + ") " + pattern_POS + "|(" + pattern_RESN3 + ") " + pattern_POS + \
    #                        "|(" + pattern_RESN3 + ")" + pattern_POS + "|(" + pattern_RESN3b + ")"
    pattern_standard = "(" + pattern_RESNF + ") " + pattern_POS + "|((" + pattern_RESN3 + ")( " + pattern_POS + \
                       "))|(" + pattern_RESN3 + ")(" + pattern_POS + ")|(,(" + pattern_RESN3 + ")" + pattern_POS + \
                       ")|(" + pattern_RESN3b + ")"

    pattern_standard_slash = "(" + pattern_RESNF + ") " + pattern_POS_slash + "|(" + pattern_RESN3 + ") " + pattern_POS_slash + \
                             "|(" + pattern_RESN3 + ")" + pattern_POS_slash + "|(" + pattern_RESN3b + ")"
    pattern_standard_dash = "(" + pattern_RESN3 + ")-" + pattern_POS

    pattern_site = "(" + pattern_standard + ") residue?" + \
                   "|((" + pattern_RESNF + "|" + pattern_RESN3 + ")(\s|,\s)in\sposition\s\d+)" + \
                   "|((" + pattern_RESNF + "|" + pattern_RESN3 + ")\sresidue(\s|,\s)at\sposition\s\d+)" + \
                   "|(" + pattern_RESNF + "|" + pattern_RESN3 + ")" + "? at position? " + pattern_POS + \
                   "|(("+ pattern_RESNF + "|" + pattern_RESN3 +") residue at position " + pattern_POS + ")" + \
                   "|(residue at position " + pattern_POS + ")" + \
                   "|((("+pattern_RESN3+")|("+pattern_RESNF+"))\sresidues(\s|,\s)at\spositions\s\d+ and \d+)" + \
                   "|((("+pattern_RESN3+")|("+pattern_RESNF+"))\sresidues(\s|,\s)at\spositions\s(\d+,)+\d+ and \d+)"

    #     pattern = "(" + pattern_standard_slash + ")" + \
    #               "|(" + pattern_standard + ")" + \
    #               "|(" + pattern_standard_dash + ")" + \
    #               "|(" + pattern_site + ")"

    pattern = "(" + pattern_RESN3dr + ")" + \
              "|(" + pattern_standard_slash + ")" + \
              "|(" + pattern_standard + ")" + \
              "|(" + pattern_standard_dash + ")" + \
              "|(" + pattern_site + ")"

    p = re.compile(pattern)

    ##Tests
    matches = []

    for match in p.finditer(fulltext):

        if verbose:
            print(match.start(), match.end(), match.group())

        m = MatchClass(match.start(), match.end(), match.group())
        m.find_position()
        m.find_amino_acid()
        matches.append(m)

    return matches


def decompose_matches(matches, encode=True, verbose=False):
    """

    Parameters
    ----------
    matches :
    encode :

    Returns
    -------
    newmatches :
    """
    newmatches = []
    for match in matches:
        if verbose:
            print(match.__dict__)
        if hasattr(match, "position") and hasattr(match, "threeletter"):
            if match.position == []:
                warnings.warn("No position found")
            else:
                if isinstance(match.position, str) or isinstance(match.position, bytes):
                    newmatch = match
                    newmatch.position = int(match.position)
                    if not hasattr(newmatch, "residue"):
                        newmatch.encode()

                    # newmatch.parent = None
                    newmatches.append(newmatch)

                elif isinstance(match.position, int):
                    newmatch = match
                    newmatch.position = match.position
                    if not hasattr(newmatch, "residue"):
                        newmatch.encode()

                    # newmatch.parent = None
                    newmatches.append(newmatch)

                elif isinstance(match.position, list):
                    if isinstance(match.threeletter, list):
                        if len(match.threeletter) == len(match.position):
                            for newthreeletter, newpos in zip(match.threeletter, match.position):
                                newmatch = MatchClass(match.start, match.end, match.string)

                                newmatch.aminoacid = aa_dict[newthreeletter]["full_id"]
                                newmatch.threeletter = newthreeletter
                                newmatch.position = newpos
                                if not hasattr(newmatch, "residue"):
                                    newmatch.encode()

                                # newmatch.parent = match.__dict__
                                newmatches.append(newmatch)

                    elif len(match.position) > 1:

                        for newpos in match.position:
                            newmatch = MatchClass(match.start, match.end, match.string)

                            newmatch.aminoacid = match.aminoacid
                            newmatch.threeletter = match.threeletter
                            newmatch.position = newpos
                            if not hasattr(newmatch, "residue"):
                                newmatch.encode()

                            # newmatch.parent = match.__dict__
                            newmatches.append(newmatch)
                    else:
                        newmatch = match
                        newmatch.position = int(match.position[0])
                        if not hasattr(newmatch, "residue"):
                            newmatch.encode()

                        # newmatch.parent = match.__dict__
                        newmatches.append(newmatch)
                else:
                    newmatch = match
                    newmatch.position = int(match.position)
                    if not hasattr(newmatch, "residue"):
                        newmatch.encode()

                    # newmatch.parent = match.__dict__
                    newmatches.append(newmatch)
        else:
            warnings.warn("Match is missing either position or threeletter attributes")

    return newmatches


def locate_residues(source, matches, decompose=True, nlp=None, cifscantype="flex",
                    verbose=True):
    """
    This function takes the raw list of matches from :func:`~pyresid.identify_residues` and augments them
    with contextual information and matches them against protein matches also found within the text.

    This is a reincarnation of locate_residues2 (despite the name).


    Parameters
    ----------
    source : :func:`~pyresid.SourceClass`
            Class instance containing the source id and fulltext (and possibly the `spaCy` doc)

    matches : List
        a list of :class:`~pyresid.MatchClass` objects

    decompose : Bool, optional, default: True.
            Whether to turn the "compound" mentions matched into actual residues.

    nlp : spaCy model - https://spacy.io/usage/models, optional, default: None
        The text model to use to turn the `source.fulltext` into `source.doc`

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    matches : List of :class:`~pyresid.MatchClass` objects
        The matches found within `fulltext`, augmented with contextual information - sentence, prefix
        postfix, protein accession id.

    """
    if not nlp:
        if verbose:
            print("loading spacy model")
        nlp = spacy.load('en')

    if not hasattr(source, "doc"):
        if verbose:
            print("making spacy doc")
        source.doc = nlp(source.fulltext)

    if decompose:
        matches = decompose_matches(matches)

    ## Find Proteins
    unique_proteins = identify_protein_ID(source.fulltext)
    accession_numbers = []
    struct_dict = {}
    ## Loop through PDB structures to load in the structures, to check the candidates against
    for prot_structure in unique_proteins:

        if verbose:
            print(prot_structure, end=" ")
        ## Many PDB entries can share a since UniProt accession URI
        accession_id = find_UniProt_accession(prot_structure)
        accession_numbers.append(accession_id)

        # Only do this if one is found!
        if accession_id:
            infile = os.path.join(mmCIF_dir, prot_structure + ".cif")

            # Only download if it is not known locally
            if not os.path.exists(infile):
                download_pdbfile(pdb_id=prot_structure, outfile=infile)

            # Load in the cif file to get the structure
            if cifscantype not in  {"flex", "standard"}:
                cifscantype = "flex"
            cf = load_pdbfile(prot_structure, infile=infile, cifscantype=cifscantype)

            res_seq = get_residue_sequence(cf)
            struct_dict[prot_structure] = res_seq

            if verbose:
                print("")

    unique_accession_numbers = set(accession_numbers)
    found_accession = [i for i in zip(unique_proteins, accession_numbers) if i[1]]

    # print(unique_proteins, unique_accession_numbers)
    # print("found accession", found_accession)
    if verbose:
        print(struct_dict.keys())
    unique_residues = set([match.residue for match in matches])

    for match in matches:
        if verbose:
            print(match.__dict__)

        ## Find Tokens?
        # w = np.logical_and(match.start >= np.array([token.idx for token in source.doc]),
        #                    match.end <= np.array([token.idx + len(token.text) for token in source.doc]))
        # if verbose:
        #     print(np.where(w))

        w = np.array([np.in1d(np.arange(token.idx, token.idx + len(token.text), 1),
                              np.arange(match.start, match.end, 1)).any() for token in source.doc])

        if verbose:
            print(np.where(w))

        # w = np.logical_and(np.array([token.idx for token in source.doc]) >= match.start,
        #                    np.array([token.idx for token in source.doc]) <= match.end)
        # if verbose:
        #     print(np.where(w))


        if len(np.where(w)[0]) > 1:
            match.token = source.doc[np.nanmin(np.where(w)):np.nanmax(np.where(w)) + 1]
            match.token_start = match.token[0].i
        else:
            match.token = source.doc[np.where(w)[0][0]]
            match.token_start = match.token.i

        for n_sent, sent in enumerate(source.doc.sents):
            if match.start >= sent.start_char and match.end <= sent.end_char:
                match.sent = sent
                match.n_sent = n_sent


        ## TODO - what happens when there is more than one mention in a sent?
        ## TODO - token.idx would be better?
        start_index = match.sent.text.index(match.string)
        match.prefix = match.sent.text[:start_index]
        match.postfix = match.sent.text[start_index + len(match.string):]

        ## Find the section
        w_section = np.digitize(match.start, [i[1] for i in source.sections])-1
        match.section = [i[0] for i in source.sections][w_section]
        ## Fuzzymatch to whitelist
        fuzzymatch = fwprocess.extract(match.section, EBI_whitelist, limit=1)[0]
        match.EBI_section = fuzzymatch[0]

        ## ASSOCIATE - MATCH TO STRUCT
        matched = []
        for k in found_accession:
            if verbose:
                print(k[0])

            if match.residue in struct_dict[k[0]]:
                if verbose:
                    print("match")
                matched.append(k[1])

        ## CHOOSE URI
        if len(matched) == 0:
            matched.append("")
            # matched.append(
            #     "NO MATCH FOUND")  ## -ASK WHETHER TO DROP UNMATCHED - see issue #2 http://hcp004.hartree.stfc.ac.uk/RobFirth/PDB-protein-res/issues/2
        match.uniprot_uri = np.unique(matched)[0]  ##Do better than just choosing the first one -
        ## duplicate entry with both URI, use ML - TODO!

    return matches


def generate_EBI_output(context_matches, source, provider="West-Life"):
    """

    Parameters
    ----------
    context_matches :
    source :
    provider :

    Returns
    -------
    output_dict :
    """
    output_dict = OrderedDict()

    output_dict["pmcid"] = "".join(filter(str.isdigit, source.ext_id))
    output_dict["provider"] = provider
    output_dict["anns"] = []

    for match in context_matches:
        annsdict = OrderedDict()

        annsdict["exact"] = match.string
        # annsdict["section"] = match.section
        annsdict["section"] = match.EBI_section
        annsdict["prefix"] = match.prefix
        annsdict["prefix"] = match.prefix

        annsdict["position"] = str(match.n_sent)+"."+str(match.token_start - match.sent.start)

        annsdict["postfix"] = match.postfix
        annsdict["tags"] = []

        tagdict = OrderedDict()

        if match.uniprot_uri == "":
            tagdict["name"] = match.residue
            tagdict["uri"] = match.uniprot_uri
        else:
            tagdict["name"] = match.uniprot_uri
            tagdict["uri"] = "https://www.uniprot.org/uniprot/" + match.uniprot_uri
            # Sameer suggested only using annotations that have solid matches to Uniprot Matches. So moving this here.
            annsdict["tags"].append(tagdict)
            output_dict["anns"].append(annsdict)

        # Sameer suggested only using annotations that have solid matches to Uniprot Matches. So commenting this out.
        # annsdict["tags"].append(tagdict)
        # output_dict["anns"].append(annsdict)

    return output_dict


def process(ext_id_list, outdir, filename="pyresid_output.json", provider="West-Life", cifscantype='flex',
            save=True, overwrite=False, return_dict=False, decompose=True, verbose=False):
    """
    This wraps the main workhorse functions, taking a list of PMC IDs and mining the resulting fulltext.
    output is a json structure (the encoded output of _locate_residues saved with MyEncoder), to match
    the EBI specifications.

    Parameters
    ----------
    ext_id_list : List of strings
         List containing ePMC identifiers used to retrieve the relevant entry. Format is prefix of 'PMC'
         followed by an integer.

    outdir : String or Path
         Directory that will contain the output file.

    filename : String
         The structured output JSON file containing the annotations, one line per `ext_id`.

    save : Bool, optional, default: True
       Flag to turn off writing the  JSON. Good for debugging whenm combined with `return_dict`.

    overwrite : Bool, optional, default: False
            Flag to determine whether to append (default) or overwrite the json file

    return_dict : Bool, optional, default: False
              Flag to return the output as a dictionary

    cifscantype : {"flex", "standard"}, default: "flex"
              Flag passed to `pycifrw` via :func:`~pyresid._locate_residues2`; `scantype` can be `standard` or `flex`.  `standard` provides
              pure Python parsing at the cost of a factor of 10 or so in speed.  `flex` will tokenise
              the input CIF file using fast C routines.  Note that running PyCIFRW in Jython uses
              native Java regular expressions to provide a speedup regardless of this argument.

    context : String, optional, default: "sent"
          Flag passed to :func:`~pyresid._locate_residues2` to determine the type of context added
          to annotations, "sent" uses the spaCy parsed sentences, anything else will use `x`
          characters either side of the matched tokens.

    verbose : Bool, optional, default: False
          Flag to turn on verbose output

    Returns
    -------
    (optional) outdict: OrderedDict
         Dictionary containing the annotations. Good for debugging.

    See Also
    --------
    * :func:`~pyresid.locate_residues`
    """
    ## TODO SQLite export?

    nlp = spacy.load('en')
    outpath = os.path.join(os.path.abspath(outdir), filename)

    if isinstance(ext_id_list, str):  ## Check that the passed list is valid (if single string, parses as single ID)
        ext_id_list = [ext_id_list, ]

    outdict = {}

    for ext_id in ext_id_list:
        print("Processing", ext_id)

        source = SourceClass()
        source.ext_id = ext_id
        source.text_dict = get_text(ext_id)                                ##retrieve the text
        source.sections = list(
            zip([source.text_dict[i]["title"] for i in source.text_dict], [source.text_dict[i]["offset"] for i in source.text_dict]))
        source.fulltext = reconstruct_fulltext(source.text_dict, tokenise=False)  ## convert from sections to fulltext

        source.doc = nlp(source.fulltext)


        matches = identify_residues(source.fulltext, verbose=verbose)            ## Find residue mentions

        context_matches = locate_residues(source, matches, decompose=decompose, nlp=nlp, cifscantype=cifscantype,
                                          verbose=verbose)

        output_dict = generate_EBI_output(context_matches=context_matches, source=source, provider=provider)

        if save:
            if overwrite:
                outflag = "w"
            else: ## otherwise append
                outflag = "a"
            with open(outpath, outflag) as outfile:
                json.dump(output_dict, outfile, cls=MyEncoder, separators=(',', ':'))
                outfile.write("\n")
        outdict[ext_id] = output_dict

        if verbose:
            print(output_dict)

    if return_dict:
        return outdict
    else:
        pass
    pass


def add_sections_to_source(source, verbose = False):
    """

    Parameters
    ----------
    source :

    verbose :


    Returns
    -------

    """
    sections = []

    indices = [token.idx for token in source.doc]

    for i, key in enumerate(source.text_dict):
        start_char = source.text_dict[key]["offset"]
        start_token_number = np.digitize(start_char, indices)-1
        start_token = source.doc[start_token_number]

        if i < len(source.text_dict) - 1:
            end_char = source.text_dict[list(source.text_dict.keys())[i+1]]["offset"] - 1
        else:
            end_char = len(source.fulltext)

        end_token_number = np.digitize(end_char, indices) - 1
        end_token = source.doc[end_token_number]

        if verbose:
            print(i, source.text_dict[key]["title"], source.text_dict[key]["offset"], end_char, np.digitize(start_char, indices)-1)

        title = source.text_dict[key]["title"]
        fuzzymatch = fwprocess.extract(title, EBI_whitelist, limit=1)[0]
        EBI_title = fuzzymatch[0]

        section = SectionMatchClass(start_char, end_char, start_token_number, start_token, end_token_number, end_token, title, EBI_title)

        sections.append(section)

    source.section_matches = sections

    return source


if __name__ == "__main__":
    pass
else:
    pass
